\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\nc{\lb}{\lambda}
\nc{\ddx}[1]{\frac{d}{d{#1}}}
\nc{\ddxt}[1]{\frac{d^2}{d{#1}^2}}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{1.5cm}

\subsection*{1}
\ssn{a}
The first derivative of $\phi_2$ with respect to $\lb$ is $\frac{\ddx{\lb}\|p(\lb)\|}{\|p(\lb)\|^2}$. Differentiating again gives a $\|p(\lb)\|^2$ in the denominator (which we don't care about because it doesn't change sign), and the numerator is
\[ \|p(\lb)\|^2\ddxt{\lb}\|p(\lb)\|-\left(\ddx{\lb}\|p(\lb)\|\right)^2\cdot2\|p(\lb)\|=\|p(\lb)\|\left(\|p(\lb)\|\ddxt{\lb}\|p(\lb)\|-2\left(\ddx{\lb}\|p(\lb)\|\right)^2\right)\]
From (4.39) in the text, we have $\|p(\lb)\|=\sqrt{\sum\frac{(q_i^Tg)^2}{(\lb_i+\lb)^2}}$. Since $(q_i^Tg)^2$ is constant wrt $\lb$, denote it by $b_i$. Then, we have
\[\ddx{\lb}\|p(\lb)\|=-\frac{\sum\frac{b_i}{(\lb_i+\lb)^3}}{\|p(\lb)\|}\]
Taking the second derivative, we have
\[-\frac{1}{\|p(\lb)\|}\cdot-3\sum\frac{b_i}{(\lb_i+\lb)^4}+\frac{1}{\|p(\lb)\|^2}\left(\sum\frac{b_i}{(\lb_i+\lb)^3}\right)\frac{\sum\frac{b_i}{(\lb_i+\lb)^3}}{\|p(\lb)\|}\]
Simplifying, this becomes
\[\frac{3}{\|p(\lb)\|}\sum\frac{b_i}{(\lb_i+\lb)^4}+\frac{1}{\|p(\lb)\|}\left(\ddx{\lb}\|p(\lb)\|\right)^2\]
so the expression inside the parens in the numerator of the second derivative of $\phi_2$ is
\[3\sum\frac{b_i}{(\lb_i+\lb)^4}-\frac{1}{\|p(\lb)\|^2}\left(\sum\frac{b_i}{(\lb_i+\lb)^3}\right)^2\]
Thus, the second derivative will be nonnegative everywhere if we have that 
\[\left(\sum\frac{b_i}{(\lb_i+\lb)^3}\right)^2-\left(\sum\frac{b_i}{(\lb_i+\lb)^4}\right)\left(\sum\frac{b_i}{(\lb_i+\lb)^2}\right)\leq0\]
Expanding out the products and subtracting, the remaining terms are of the form 
\[\frac{2b_ib_j}{(\lb_i+\lb)^3(\lb_j+\lb)^3}-\frac{b_ib_j}{(\lb_i+\lb)^4(\lb_j+\lb)^2}-\frac{b_ib_j}{(\lb_i+\lb)^4(\lb_j+\lb)^2}\]
Factoring out the $b_ib_j$ (they're all positive), we're left with an expression of the form $2a^3b^3-a^4b^2-a^2b^4$, where $a,b\geq0$. In turn, this can be written as $-(a-b)^2a^2b^2\leq0$, which means that the whole thing is less than $0$, which means that the second derivative is nonnegative.
\ssn{b}
%Expressing the ``tangent below the graph'' property in the terms of this problem, we have that $\phi_2(\lb_l)-\phi_2'(\lb_l)(\lb_{l+1}-\lb_l)\leq\phi_2(\lb_{l+1})$. Substituting in $\lb_{l+1}=\lb_l-\frac{\phi_2(\lb_l)}{\phi_2'(\lb_l)}$, we have
%\[\phi_2(\lb_l)-\phi_2'(\lb_l)\left(-\frac{\phi_2(\lb_l)}{\phi_2'(\lb_l)}\right)\leq\phi_2(\lb_{l+1})\]
We know that $\phi_2$ is nonincreasing, so $\phi_2'(\lb)\leq0$ everywhere. If $\lb_{l}<\lb^*$, then $\phi_2(\lb_l)>0$, so $\lb_{l+1}=\lb_l-\frac{\phi_2(\lb_l)}{\phi_2'(\lb_l)}>\lb_l$. Then, using the ``tangent-below-the-graph'' property $\phi_2(\lb_l)+\phi_2'(\lb_l)(\lb^*-\lb_l)\leq\phi_2(\lb^*)=0$, we have that $\phi_2'(\lb_l)\leq-\frac{\phi_2(\lb_l)}{\lb^*-\lb_l}$, so that $\lb_l-\frac{\phi_2(\lb_l)}{\phi_2'(\lb_l)}\leq\lb_l+\frac{\phi_2(\lb_l)}{\frac{\phi_2(\lb_l)}{\lb^*-\lb_l}}=\lb^*$.

Otherwise if $\lb_l>\lb^*$, then the inequality $\phi_2(\lb_l)+\phi_2'(\lb_l)(\lb^*-\lb_l)\leq0$ turns into $\phi_2'(\lb_l)\geq-\frac{\phi_2(\lb_l)}{\lb^*-\lb_l}$. Since we now have $\phi_2(\lb_l)<0$, then $\lb_l-\frac{\phi_2(\lb_l)}{\phi_2'(\lb_l)}\leq\lb_l+\frac{\phi_2(\lb_l)}{\frac{\phi_2(\lb_l)}{\lb^*-\lb_l}}=\lb^*$
\ssn{c}
First, note that the ``otherwise'' condition will be hit only finitely times -- if the initial point is to the right of $\lb^*$, then that condition will be executed until $\tl{\lb}^{l+1}$ ends up to the right of $-\lb_1$ or until the iteration of the second condition takes $\lb^l$ to the left of $\lb^*$ (which is guaranteed to occur in a finite number of steps). Then, once the point is in $(-\lb_1,lb^*)$, all subsequent points will lie in that interval also by (b). Thus, all we need to worry about in the tail of the sequence is the Newton's method updating.

We already know that the algorithm converges to something in $(-\lb_1, \lb^*]$ due to the sequence being monotonic and bounded. Suppose that the sequence converges to something less than $\lb^*$, call it $x^*$. Let $\phi_2(x^*)=M>0$, and let $N$ be a uniform bound on $|\phi_2'(\lb)|$ on some open interval $I$ containing $x^*$. Then, we have for any $\lb^l\in I$, $\lb^l<x^*$ that $\lb^{l+1}=\lb^l+\frac{\phi_2(\lb^l)}{|\phi_2(\lb^l)|}\geq\lb^l+\frac{M}{N}$. Then, for $\lb^l$ sufficiently close to $x^*$, $\lb^{l+1}>x^*$, contradicting convergence.

To show quadratic convergence, for any $l$, we have by Taylor's theorem that
\[0=\phi_2(\lb^*)=\phi_2(\lb^l+(\lb^*-\lb^l))=\phi_2(\lb^l)+\phi_2'(\lb^l)(\lb^*-\lb^l)+\frac{1}{2}(\lb^*-\lb^l)^2\phi_2''(t)\]
for some $t\in[\lb^l,\lb^*]$. Then, dividing by $\phi_2'(\lb^l)(\lb^*-\lb^l)^2$ and rearranging gives
\[0=\frac{\frac{\phi_2(\lb^l)}{\phi_2'(\lb^l)}+\lb^*-\lb^l}{(\lb^*-\lb^l)^2}+\frac{\phi_2''(t)}{\phi_2'(\lb^l)}\]
The first term is the ratio that we care about -- we want to show that it's bounded above. Fortunately, the second derivative is uniformly bounded above on $(\lb^k, lb^*)$ where $\lb^k$ is the first iterate in $(-\lb_1,lb^*)$, and the magnitude of first derivative is uniformly bounded below on the same interval. Thus, we have quadratic convergence.
\ssn{d}
To solve the subproblem, we are given a matrix $B$ and the value of the gradient and the function at the particular point. The characterization theorem gives us necessary and sufficient criteria for a solution. First, we want to check to see whether there is a solution with $\|p\|<\Delta$. If this is to be true, then we must have $\lb=0$, which means that $B$ must be pos. semidef. and there is a solution to $Bp=-g$. Since we're going to be using the eigenvalue decomposition, we can achieve this step as follows:

\begin{algorithmic}
    \State let $QDQ^T=B$
    \If{all entries of $D$ are nonnegative}
        \State attempt to compute a solution to $DQ^Tp=-Q^Tg$
        \If{the zeros in $D$ are in the same place as in $Q^Tg$}
            \If{the solution has norm less than or equal to $\Delta$}
                \State return solution
            \EndIf
        \EndIf
    \EndIf
    \State assert that $\|p\|=\Delta$ and proceed to iterative solution
\end{algorithmic}

We also need to ensure that $q_1^Tg\neq0$. If this doesn't hold, then we can just use (4.45) to compute $p$ and be done with it. Now, for the iteration, we have

\begin{algorithmic}
    \State Partition $Q$ by columns into $Q_1$ and $Q_2$, with $Q_1$ containing all eigenvectors with the lowest eigenvalue.
    \State Let $D$ be the array of eigenvalues in increasing order. Partition it similarly to $Q$.
    \If{$Q_1^Tg=0$}
        \State $u\gets Q_2^Tg$
        \State $d\gets D_2+\lambda_1$
        \State $\tau\gets\sqrt{\Delta-\sum\left(\frac{u_i}{d_i}\right)^2}$
        \State return $\sum_{i\in Q_2}\left(\frac{u_i}{d_i}\right)q_i+\tau Q_1(1)$
    \EndIf

    \State Initialize $\lb=\lb^0$, $c$.
    \While{$|\phi_2(\lb)|>\ep$}
        \State Compute $\phi_2(\lb)$ and $\phi_2'(\lb)$ according to formulas from (a)
        \State $\tl{\lb}\gets\lb-\frac{\phi_2(\lb)}{\phi_2'(\lb)}$
        \If{$\tl{\lb}\leq-\lb_1$}
            \State $\lb\gets-c\lb_1+(1-c)\lb$
        \Else
            \State $\lb\gets\tl{\lb}$
        \EndIf
    \EndWhile
    \State return $-\sum_{i=1}^n\frac{q_i^Tg}{\lambda_i+\lambda}q_j$
\end{algorithmic}
\subsection*{2}
The backtracking approach gives us a pair of inequalities. The first is identical to the Goldstein conditions:  $f(x_k+\alpha p_k)\leq f(x_k)+c\alpha\nabla f_k^Tp_k$. The second comes from the termination of the iteration: $f(x_k+\alpha p_k/\rho)>f(x_k)+c\alpha\nabla f_k^Tp_k/\rho$, where $\rho\in(0,1)$. 

Define $g(t)=f(x_k+t\alpha p_k/\rho)$. Then $g(0)=f(x_k)$ and $g(1)=f(x_k+\alpha p_k/\rho)$. We have 
\[g(1)-g(0)=\int_0^1\frac{d}{dt}g(t)dt=\int_0^1\alpha\nabla f(x_k+t\alpha p_k/\rho)^Tp_k=\frac{\alpha}{\rho}\nabla f(x_k+t\alpha p_k/\rho)^Tp_k\]
for some $t\in[0,1]$, the latter equality by the mean value theorem. Plugging this into the second inequality from backtracking, we have
\[\alpha\nabla f(x_k+t\alpha p_k/\rho)^Tp_k>c\alpha\nabla f(x_k)^Tp_k\]
Subtracting $\alpha\nabla f(x_k)^Tp_k$ from both sides and collecting terms, we have
\[\alpha_k\left(\nabla f(x_k+t\alpha p_k/\rho)-\nabla f(x_k)\right)^Tp_k>\alpha(c-1)\nabla f(x_k)^Tp_k\]
Since the RHS is positive, we can apply Cauchy-Schwarz then the Lipschitz condition to the LHS to get
\[\frac{L\alpha^2t}{\rho}\|p_k\|^2>\alpha(c-1)\nabla f(x_k)^Tp_k\]
Since $t\in[0,1]$, we can discard it. Rearranging so that we get a bound on $\alpha$, we have 
\[\alpha>\frac{\rho(c-1)\nabla f(x_k)^Tp_k}{L\|p_k\|^2}\]
Then, plugging this into the first inequality gives us
\[f(x_k)-f(x_{k+1})\geq-c\alpha\nabla f(x_k)^Tp_k>-\frac{\rho(c^2-c)\left(\nabla f(x_k)^Tp_k\right)^2}{L\|p_k\|^2}\]
Now since $a^Tb=\|a\|\|b\|\cos(\theta)$, we have that the above is equal to 
\[\frac{\rho(c-c^2)\|\nabla f(x_k)\|^2\cos^2(\theta_k)}{L}\]
Thus, the sum of the above expression over all $k$ is bounded above by the sum of $f(x_k)-f(x_{k+1})$ over all $k$(ignoring the constant terms $\rho$ and $c$), which is a bounded telescoping sum due to the below-boundedness of $f$. Thus, we have convergence.
\subsection*{3}
We know from the structure of the trust region and line search algorithms that the sequence of function values is monotonic -- $f(x_{k+1})\leq f(x_k)$ for all $k$. The second part of the assumption gives us that there is a unique, isolated minimizer $x^*$ somewhere in $L$. Then, we know that if $f(x_k)$ converges to $f(x^*)$, we have by continuity of $f$ that $x_k$ converges to $x^*$.

Translate the function such that $f(x^*)=0$. Suppose that $f(x_k)$ does not converge to $0$. Then, since the sequence of function values is monotonic, we must have that $f(x_k)$ is bounded below by some $\ep>0$, which means that all $x_k$ remain outside the set $\{x: f(x)\leq\ep/2\}$. However, since there is a limit point $x^\sharp$ of the $x_k$ with $\|\nabla f(x^\sharp)\|=0$, this means that $f(x^\sharp)\geq\ep/2$ and $\|\nabla f(x^\sharp)\|=0$, which contradicts the strong convexity that the assumption gives us.
\subsection*{4}
\ssn{a}
Differentiating $P(x)$ wrt $d$, we have $f'(x)+df''(x)+\frac{1}{2}d^2f'''(x)$. Finding the roots of this (the stationary points) with the quadratic formula, we have 
\[d=\frac{-f''(x)+\sqrt{\left(f''(x)\right)^2-2f'(x)f'''(x)}}{f'''(x)}\]
As $x\to x^*$, the term inside the square root approaches $\left(f''(x)\right)^2$ because $f'''(x)$ and $f''(x)$ are both bounded both above and away from zero near $x^*$. Thus, we have a stationary point $d$ that approaches zero as $x\to x^*$
\ssn{b}
\end{document}

