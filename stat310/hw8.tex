\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage{mathrsfs}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{1.5cm}
\subsection*{1}
\ssn{a}
The \verb|pcg| function accepts function pointers for the matrix $A$, so we can define a function $Q(x)$ that computes $Lx$ and adds it to $w\cdot(w^Tx)$. Since $L$ and $w$ have $O(n)$ entries, this computation should take $O(n)$ time per iteration. Additionally, if we use $L$ as the preconditioning matrix, its tridiagonal structure allows for the solve step to be done in $O(n)$ steps as well.
\ssn{b}
For all these, the $L$ matrix is precomputed and stored by way of persistent variables, so the time necessary for its computation is not included. 

The preconditioned CG was done with $M=L=CC^T$, $C$ being the lower-triangular factorization of $L$. This is because transforming the matrix $CC^T+ww^T$ via $C^{-1}(CC^T+ww^T)C^{-T}$ gives $I+(C^{-1}w)(C^{-1}w)^T$, which is a rank-1 update to the identity, which has only two distinct eigenvalues. Thus, we expect CG to converge extremely quickly in this case (two iterations). 

For the direct solution via Cholesky factorization, the directive was to use sparse linear algebra when possible, and the best way to do this was to use the Sherman-Morrison formula to compute $(L+ww^T)^{-1}f$. This gives us
\[L^{-1}b-\frac{(w^TL^{-1}f)L^{-1}w}{1+w^TL^{-1}w}\]
The vectors $L^{-1}w$ and $L^{-1}f$ can each be computed in $O(n)$ time by taking the Cholesky factorization of $L$ first and backsolving, and the rest of the vector-vector operations take about $8n$ operations, which actually seems considerably cheaper than the preconditioned CG method. In fact, if we look at the timing results, the direct solution was the fastest in all cases.

For the data collection, I modified the set of sizes to be \verb|[400, 600, 1000, 3000, 5000, 10000]|. The time reported is the average of three runs. The plots for the methods follow, with un-preconditioned on left, pre-conditioned on right, and the direct solution below.

\includegraphics[width=0.5\textwidth]{hw8_files/1_un_pre.png}
\includegraphics[width=0.5\textwidth]{hw8_files/1_pre.png}

\includegraphics[width=0.5\textwidth]{hw8_files/1_direct.png}

It looks like the un-preconditioned CG method's time grows superlinearly, while the preconditioned CG and direct method's times grow linearly. The large amount of time that the unpreconditioned method took was not surprising -- the eigenvalues of $L$ are pretty spread out, and \verb|pcg| reported taking $O(n)$ iterations to converge. 
The weird jump from $1000$ to $3000$ in the preconditioned CG is due to the fact that the method suddenly started taking 3 iterations for some reason -- it's probably due to numerical issues, since it was noted above that theoretically it should only take two iterations. Additionally, the speed of the direct solution was expected -- using the Sherman-Morrison formula to reduce everything into one factorization and two sparse backsolves seems very efficient.
\ssn{c}
I think I explained this above in the remark about the eigenvalues of $CC^T+ww^T$ after transforming the problem.
\subsection*{2}
\ssn{a}
To modify the L-BGFS algorithm, replace the Wolfe step with a backtracking step, then use the damping algorithm to compute some $r_k$ according to (18.15) and store that in place of the $y_k$. Since the damped update works simply by replacing $y_k$ with $r_k$, this modification should carry over into the limited memory version as well, where all we're doing differently is throwing away older information.

The hard part, then, comes from computing $r_k$, which involves computing the matrix-vector product $B_ks_k$, which we cannot do with the algorithm for computing $H_kx$ that we were given. Fortunately, the update formula for $B_k$ is much simpler than that for $H_k$ -- if we store the vector $c_i=B_is_i$ for $i=k-m,\ldots,k-1$, the formula for $B_k$ is simply 
\[B_k^0-\frac{c_{k-m}c_{k-m}^T}{s_{k-m}^Tc_{k-m}}+\frac{r_{k-m}r_{k-m}^T}{s_{k-m}^Tr_{k-m}}-\cdots-\frac{c_{k-1}c_{k-1}^T}{s_{k-1}^Tc_{k-1}}+\frac{r_{k-1}r_{k-1}^T}{s_{k-1}^Tr_{k-1}}\]
For an initial guess of $B_k^0$, we can just use $\frac{1}{\gamma_k}I$, where $\gamma_k$ is define as in (7.20) in the text. The book didn't bother giving much justification for their choice of $H_k^0$, so I'm just going to justify this choice of $B_k^0$ by noting that it's the inverse of $H_k^0$.
\subsection*{3}
Since preconditioned CG is derived from un-preconditioned CG by applying the un-preconditioned algorithm to the problem $(C^{-T}AC^{-1})x=C^{-T}b$, writing that algorithm gives us the initial points $r_0=(C^{-T}AC^{-1})x_0-C^{-T}b$ and $p_0=-r_0$, and the body of the loop constitutes
\begin{align*}
    \alpha_k&=\frac{r_k^Tr_k}{p_k^TC^{-T}AC^{-1}p_k}\\
    x_{k+1}&=x_k+\alpha_kp_k\\
    r_{k+1}&=r_k+\alpha_kC^{-T}AC^{-1}p_k\\
    \beta_{k+1}&=\frac{r_{k+1}^Tr_{k+1}}{r_k^Tr_k}\\
    p_{k+1}&=-r_{k+1}+\beta_{k+1}p_k\\
\end{align*}
Substituting $p_k'=C^{-1}p_k$ for all $k$ (including zero), we have the initial point $p_0'=-C^{-1}r_0$ and the loop
\begin{align*}
    \alpha_k&=\frac{r_k^Tr_k}{p_k'^TAp_k}\\
    x_{k+1}&=x_k+\alpha_kCp_k'\\
    r_{k+1}&=r_k+\alpha_kC^{-T}Ap_k'\\
    \beta_{k+1}&=\frac{r_{k+1}^Tr_{k+1}}{r_k^Tr_k}\\
    p_{k+1}'&=-C^{-1}r_{k+1}+\beta_{k+1}p_k'\\
\end{align*}
Now, substituting $r_k'=C^Tr_k$ and $x_k'=C^{-1}x_k$ gets the initial conditions $r_0'=Ax_0'-b$ and $p_0'=-C^{-1}C^{-T}r_0$. Letting $y_k\equiv C^{-1}C^{-T}r_k'\equiv M^{-1}r_k'$, we have the loop
\begin{align*}
    \alpha_k&=\frac{r_k'^TC^{-1}C^{-T}r_k'}{p_k'^TAp_k'}=\frac{r_k'^Ty_k}{p_k'^TAp_k}\\
    x_{k+1}'&=x_k'+\alpha_kp_k'\\
    r_{k+1}'&=r_k'+\alpha_kAp_k'\\
    \beta_{k+1}&=\frac{r_{k+1}'^TC^{-1}C^{-T}r_{k+1}'}{r_k'^TC^{-1}C^{-T}r_k'}=\frac{r_{k+1}'^Ty_{k+1}}{r_k'^Ty_k}\\
    p_{k+1}'&=-C^{-1}C^{-T}r_k'+\beta_{k+1}p_k'=-y_k+\beta_{k+1}p_k'\\
\end{align*}
which is exactly the same as the algorithm for preconditioned CG. Thus, we have the condition $r_i^Tr_j=0$ for all $i\neq j$ from regular CG translating to $r_i'^TC^{-1}C^{-T}r_j'=r_i'^TM^{-1}r_j'=0$, which is the condition we desired.
\subsection*{4}
For the Jacobian, use the Frobenius norm. We want to find a constant $M$ such that $\|J(x)-J(y)\|_F^2\leq M^2\|x-y\|^2$. By the structure of the Jacobian (10.3), we have that 
\[\|J(x)-J(y)\|_F^2=\sum_{i=1}^m\|\nabla r_i(x)-\nabla r_i(x)\|^2\leq\sum_{i=1}^mL^2\|x-y\|=mL^2\|x-y\|\]
so we can take $M=\sqrt{m}L$.

For the gradient of the function, we have
\[\norm{\nabla f(x)-\nabla f(y)}=\norm{\sum_{i=1}^mr_i(x)\nabla r_i(x)-r_i(y)\nabla r_i(y)}\leq\sum_{i=1}^m\norm{r_i(x)\nabla r_i(x)-r_i(y)\nabla r_i(y)}\]
Since the set we're concerned with is compact, let $\|r_i(x)\|$ and $\|\nabla r_i(x)\|$ be bounded above by some $W$ for all $i$, $x\in\mathscr{D}$. Then, the summand is
\begin{align*}
    \norm{r_i(x)\nabla r_i(x)-r_i(y)\nabla r_i(y)+r_i(x)\nabla r_i(y)-r_i(x)\nabla r_i(y)}&=\norm{r_i(x)(\nabla r_i(x)-\nabla r_i(y))+(r_i(x)-r_i(y))\nabla r_i(y)}\\
                                                                                          &\leq\|r_i(x)\|\|\nabla r_i(x)-\nabla r_i(y)\|+\|r_i(x)-r_i(y)\|\|\nabla r_i(y)\|\\
                                                                                          &\leq 2WL\|x-y\|\\
\end{align*}
so overall, we have that $\norm{\nabla f(x)-\nabla f(y)}\leq2mWL\|x-y\|$.
\end{document}
