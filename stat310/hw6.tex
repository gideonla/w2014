\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\nc{\pd}{\partial}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{1.5cm}
\subsection*{1}
\ssn{1}
The sampling function will take $X$ as a $d$ by $n$ matrix, $\theta$ as a $d$ by $1$ column vector, and $r$, the number of samples to return. It will return an $n$ by $r$ matrix with the columns as the samples.
\ssn{2}
First, we have that 
\[\frac{\partial K_{ij}}{\partial \theta_l}=\frac{\partial k_\theta(x_i, x_j)}{\partial r_{ij}}\frac{\partial r_{ij}}{\partial \theta_l}\]
The derivative by $r_{ij}$ is $-\frac{5}{3}r_{ij}e^{-r_{ij}\sqrt{5}}(1+r_{ij}\sqrt{5})$, and the derivative by $\theta_l$ is $-\frac{(x_{li}-x_{lj})^2}{\theta_l^3r_{ij}}$, so multiplying the two together gives
\[\frac{5(x_{li}-x_{lj})^2}{3\theta_l^3}e^{-r_{ij}\sqrt{5}}(1+r_{ij}\sqrt{5})\]
Let $m_{ijl}=\frac{5(x_{li}-x_{lj})^2}{3\theta_l^3}$. Then, we have that
\[K_{ij}=\frac{\partial K_{ij}}{\partial \theta_l}\frac{1}{m_{ijl}}+\frac{5}{3}r_{ij}^2e^{-r_{ij}\sqrt{5}}\]
which gives us an expression for the derivative of $K_{ij}$. Forming the matrix $\frac{\pd K}{\pd \theta_l}$ will then take $O(n^2)$ effort for fixed $d$, which is fine because $d$ is expected to be small.

Now, we are interested in evaluating the log-likelihood and its gradient. If we fix $\theta$ and $X$, then we fix $K$, and we take its Cholesky decomposition once as $K=LL^T$. In the log-likelihood, the first term can easily be computed by backsolving twice to get $K^{-1}y$ then multiplying with $y^T$ on the left. The determinant term is the product of the diagonal on $L$, and the third term is a constant. 

In the gradients, we already have $K^{-1}y$ from the function evaluation, so then the first term is just a matter of performing a matrix-vector multiplication. The second term is a bit more difficult, where we have to calculate the trace of $L^{-T}L^{-1}\frac{\pd K}{\pd\theta_l}$, but executing \verb|L.'\(L\Kd)| only takes $0.08$ seconds on my system, so that probably won't be a problem in terms of execution time. 

After implementing this, it turns out that calculating the $K$ matrix itself and its derivatives takes an inordinate amount of time due to the un-vectorizable formula for computing them, so I ended up writing the hot loops in C and using mex to stick them into the code. The C files are attached, and the original Matlab functions are still there. I tested them and the output is the same; the only difference is in runtime.
\ssn{3}
For this part and the next, I'm assuming that we're actually supposed to take 10 points in $[0,3]$, with the first point being 0 and the last point being 3. In that case, the increment between points is $1/3$, and I'll follow that for the next part too, in which case the increment will be $3/2$ and $3/5$ for the two cases, respectively.
\subsection*{2}
\ssn{1}
\subsection*{3}
Assume that $B_k=H_k^{-1}$. Then, the inverse of $B_{k+1}=B_k+\frac{r_kr_k^T}{r_k^Ts_k}$, where $r_k=y_k-B_ks_k$, is, by the Sherman-Woodson formula, 
\[H_k-\frac{\frac{H_kr_kr_k^TH_k}{r_k^Ts_k}}{1+\frac{r_k^TH_kr_k}{r_k^Ts_k}}\]
Now, noting that $H_kr_k=H_ky_k-s_k$, we have
\[H_k-\frac{(H_ky_k-s_k)(H_ky_k-s_k)^T}{r_k^Ts_k+r_k^T(H_ky_k-s_k)}\]
Now, we have in the denominator $r_k^TH_ky_k=(H_kr_k)^Ty_k=(H_ky_k-s_k)^Ty_k$. Canceling out the negatives in the numerator and taking the negative out of the denominator, we end up with (6.25).
\subsection*{4}
\ssn{a}
Given $x, y$, assume that they're both nonzero (otherwise the problem is trivial), and find some set of vectors $w_1,\ldots,w_{n-1}$ which form a basis when taken together with $x$. Define $Q$ as the matrix formed by taking this basis as columns. Then, we have $Qe_1=x$ so $Q^{-1}x=e_1$, and therefore $\det(I+xy^T)=\det(Q^{-1}(I+xy^T)Q)=\det(I+e_1y^TQ)$. The matrix $e_1y^TQ$ has the first row as $y^TQ$ and zero elsewhere, so adding this to $I$ produces an upper triangular matrix whose determinant is the product of the diagonal, or $1$ plus the first entry of $y^TQ$. This first entry is $y^TQe_i=y^Tx$, so we have that the determinant is $1+y^Tx$.
\ssn{b}
First suppose that $x$ and $u$ are linearly independent. If not, then factor out a scalar and reduce to the previous problem. Then, define the $Q$ matrix with its first column as $x$, second column as $u$, and the other columns as a completion of the basis. We then get that $Qe_1=x$ and $Qe_2=u$, so similarly to above, we can write $\det(I+xy^T+uv^T)=\det(I+e_1y^TQ+e_2v^TQ)$. The first and second coordinates of $y^TQ$ are $y^Tx$ and $y^Tu$, and the first and second coordinates of $v^TQ$ are $v^Tx$ and $v^Tu$, respectively. Then, the matrix we're taking the determinant of looks like this:
\[\openm 1+y^Tx&y^Tu&y^TQe_3&\hdots&&y^TQe_n\\
v^Tx&1+v^Tu&v^TQe_3&\hdots&&v^TQe_n\\
0&0&1&0&\hdots&0\\
\vdots&\vdots&&\ddots\\
0&0&\hdots&&&1\\
\closem\]
Expand by minors along the first column. We then get $(1+y^Tx)(1+v^Tu)-(v^Tx)(y^Tu)$ as desired.
\ssn{c}
We have 
\[B_k^{-1}B_{k+1}=I-\frac{s_k(B_ks_k)^T}{s_k^TB_ks_k}+\frac{B_k^{-1}y_ky_k^T}{y_k^Ts_k}\]
Then, the determinant of the LHS is $\frac{\det(B_{k+1})}{\det(B_k)}$, and we can apply the above formula to the RHS with $x=\frac{s_k}{s_k^TB_ks_k}$, $y=-B_ks_k$, $u=B_k^{-1}y_k$, and $v=\frac{y_k}{y_k^Ts_k}$ to obtain
\[\left(1-\frac{s_k^TB_ks_k}{s_k^TB_ks_k}\right)\left(1+\frac{y_k^TB_k^{-1}y_k}{y_k^Ts_k}\right)-\left(\frac{y^Ts_k}{y_k^Ts_k\cdot s_k^TB_ks_k}\right)\left(-s_k^TB_k^{-1}B_ky_k\right)
\]
The first term goes away entirely, and the second term simplifies to $\frac{s_k^Ty_k}{s_k^TB_ks_k}$, which gives us what we want after multiplying out by $\det(B_k)$.
\end{document}
