\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{1.5cm}
\subsection*{1}
\ssn{a}
If we wish to use the trust-region framework to solve this problem, the Hessian approximation in the problem is only a $9\times9$ matrix, which means that any reasonable operations on it will be very cheap and take an insignificant amount of effort compared to the effort of actually evaluating the function/Jacobian. Thus, a good way to solve the subproblem exactly would be to use the eigendecomposition method from (1) of the midterm, mostly since it's already been implemented. 

Unfortunately, I was not able to get this algorithm to converge in any reasonable amount of time when not using the quasi-newton enhancement and with the provided starting point (I remember it being said that this problem is very sensitive to the choice of starting point). Thus, instead, I found a starting point that works, and ran the unenhanced algorithm from that starting point. The point in question is 
$[0.4789,3.0474,0.5442,0.0146,-1.6093,0.5419,0.0514,0.2381,1.5101]$
and the other parameters were $\h{\Delta}=1$, $\eta=0.2$, $c=0.5$, and the stopping criterion was $\|Jr\|_2<\ep=10^{-3}$. This converged in 
\ssn{b}
For this part, the quasi-newton enhancement did make the algorithm converge faster than plain LM for the above starting point, but it was difficult to see any evidence of superlinear convergence there. However, starting from the point provided on chalk with other parameters the same as above except $\ep=10^{-5}$, the plot of ratios of successive differences looks like

\includegraphics[width=0.5\textwidth]{final_files/p1/1b_superlinear.png}

which does indicate that there is evidence of superlinear convergence.
\ssn{c}
Here is the plot of the fit superimposed over the data:

\includegraphics[width=\textwidth]{final_files/p1/1c_fit_data.png}

It's kinda hard to tell if there's a good fit, but the annual pattern seems to fit fairly well, and if I squint hard enough, I can sorta see the longer-term trends lining up. Here are the residuals:

\includegraphics[width=\textwidth]{final_files/p1/1c_resid_index.png}

It's hard to tell, but it looks like there might be another wavy pattern in there, which means that there might be another cycle that the model didn't account for. The best-guesses for the periods of the two cycles are the reciprocals of $\h{\beta}_4$ and $\h{\beta}_7$, which are $44.3$ months and $26.9$ months, respectively. (this is starting from the point given on chalk).
\subsection*{2}
In the CG iteration, the remaining inequality constraints were dealt with by simply stopping the iteration and returning when the current point strayed beyond one of the bounds. Execution times for $n$ up to 100 are below. After running through the profiler, it turns out that most of the time was spent in the hot loop in \verb|get_active_bnds|, which is kinda unfortunate because otherwise I'd be able to work on much bigger values of $n$. Stopping conditions simply amount to checking that the inactive coordinates of the gradient are zero or close to so.

For speeding up the iteration, I decided to use the ideal preconditioner ($Z^TGZ$) because $Z$ is so simple that this product is as easy as slicing $G$, and $G$ is so sparse that doing the solve isn't very expensive. It also seems to have the added benefit of not straying outside the inequality bounds so that the CG loop gets to a point way closer to the optimal point, which in turn reduces the number of gradient projections, which are the thing that takes up the most time.

\begin{tabular}{c|cccccc}
    $n$&4&8&16&32&64&100\\
    \hline
    No preconditioner&0.2568&0.0166&0.0414&0.1984&1.7779&7.8181\\
    \hline
    Preconditioned&0.0020&0.0029&0.0080&0.0407&0.4883&2.0842\\
\end{tabular}

Note that this algorithm produces a monotone increase in the size of the active set of constraints -- once the gradient projection finds a local minimum that's constrained on an additional coordinate, the subsequent CG iteration will keep it within that constrained surface, and subsequent starting points will also be located on that constrained surface. Thus, once we get to the correct set of active constraints, the CG iteration should be able to hone in on the optimal point. 

Convergence, then, depends on the gradient projection eventually getting the active set right. For the sake of simplicity, suppose that there is no additional CG step and that subsequent projection steps continue where the last one found a local min. Then, in order to not end up with the correct active set, we'd have to get stuck on a too-small active set, which means that a local min is consistently found before the first breakpoint. However, this looks a lot like steepest descent line search, which we know to converge.

Finally, here's a picture with $n=200$ done by the preconditioned version:

\includegraphics[width=\textwidth]{final_files/p2/obstacle_200.png}
\subsection*{3}
\ssn{a}
For this problem, I'm choosing to use the interior point method outlined in section 16.6 and solving the problem in its primal form. First, in the primal form, the problem is particularly simple, with $G$ being mostly the identity and $b$ zeros and $C$s. Second, it seems like a more direct approach than the augmented Lagrangian method, as the latter involves an extra layer of iteration (granted, with preconditioning in place, solving the box-bounded problem doesn't take many repeats, but it's still extra work). 

Reformulating the problem in standard constrained QP format, let $r=\openm w\\b\\\xi\closem\in\rn^{m+n+1}$, with $w\in\rn^m$, $b\in\rn$, and $\xi\in\rn^n$. Then, the problem becomes 
\begin{align*}
    \text{Minimize }f(r)&=\frac{1}{2}r^TGr+r^Tc,\ \text{where }G=\openm I_n&0\\0&0\closem\text{ and }c=\openm1_n\\0_n\closem\\
    \text{subject to }Ar&\geq\openm1_n\\0_n\closem\\
    \text{where }A&=\openm y_1x_1^T&y_1&e_1\\y_2x_2^T&y_2&e_2\\\vdots&\vdots&\vdots\\y_nx_n^T&y_n&e_n\\0&0&e_1\\0&0&e_2\\\vdots&\vdots&\vdots\\0&0&e_n\closem\\
\end{align*}
By the form of $A$, it seems like it probably won't be worth it to do this problem in sparse linear algebra, seeing as the upper left block of $A$ is pretty dense. 

My intent is to follow the framework in algorithm (16.4) in the textbook, with equal step lengths and modification of the provided starting point. The work then is in determining how to solves the systems to obtain the affine scaling step and the actual step. 

In this problem, we have that $m=123$, so that even if we take $n=1000$, we have that the length of $x$, $y$, and $\lambda$ put together is only $5123$. If we use the normal equations form as in (16.62), the resulting matrix we need to perform a solve on has the same shape as $G$, or $1124\times1124$. This seems like a fairly managable operation, seeing as an LDL factorization on matrices around this size take less than a second. However, larger values of $n$ might not do so well, as taking $n=30000$ would be pushing the limits on the size of machine memory, especially on the storage of $A$. In that case, a projected CG approach like the book suggests might do better.

Here, though, I'm just going to go the route of the direct solution. To solve for the affine scaling step, we solve the system
\[(G+A^T\mathscr{Y}^{-1}\Lambda A)\Delta x^\text{aff}=-r_d+A^T\mathscr{Y}^{-1}\Lambda(-r_p-y)\]
where $r_d$ and $r_p$ are defined similarly as in the book. To obtain $\Delta\lambda^\text{aff}$, compute $\Lambda\mathscr{Y}^{-1}(y+A\Delta x^\text{aff}+r_p)$, and similarly $\Delta y^\text{aff}=A\Delta x^\text{aff}+r_p$. 

To solve for the actual step, transform (16.67) to the a form similar above to obtain
\[(G+A^T\mathscr{Y}^{-1}\Lambda A)\Delta x=-r_d+A^T\mathscr{Y}^{-1}(r_l-\Lambda r_p)\]
where $r_l$ is the last entry on the RHS of (16.67). Then, we have that $\Delta\lambda=\mathscr{Y}^{-1}(r_l-\Lambda A\Delta x-\Lambda r_p)$ and $\Delta y=A\Delta x+r_p$.

Due to the diagonal nature of all of these matrices except for $A$, the code will not contain explicit matrix-vector multiplications for the most part.

As for stopping conditions, we just need to make sure we satisfy the KKT conditions (16.55). For some provided $\ep$, the stopping condition is if (16.55a,b,c) all have norm less than $\ep$.
\ssn{b}
For all the timing results below, I took $C=200$ and $\ep=10^{-3}$ (since lower values of $C$ seemed to produce faster convergence rates). 

\begin{tabular}{c|ccccccc}
    $n$&200&400&600&800&1000&1500&2000\\
    Time&0.3288&1.5629&3.8450&7.4499&17.2098&49.1102&97.1836\\
\end{tabular}

Using the hyperplane parameters from the $n=2000$ case, I classified the rest of the provided data set and compared them to the actual labels. The misclassified examples constituted about $16\%$. Looking up the a9a data set online, I found various measures of accuracy on this data set, which were mostly around the $84\%$ that I managed to get. The $w$ and $b$ parameters from the $n=2000$ run are stored in a file called \verb|p3_params.csv| in \verb|w2014:stat310/final_files/p3| on my GitHub (https://github.com/hallliu/w2014), since it would be somewhat pointless to print these numbers out. $w$ is the first 123 entries, $b$ is the last entry.
\end{document}
