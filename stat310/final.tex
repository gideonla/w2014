\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{1.5cm}
\subsection*{1}
\ssn{a}
If we wish to use the trust-region framework to solve this problem, the Hessian approximation in the problem is only a $9\times9$ matrix, which means that any reasonable operations on it will be very cheap and take an insignificant amount of effort compared to the effort of actually evaluating the function/Jacobian. Thus, a good way to solve the subproblem exactly would be to use the eigendecomposition method from (1) of the midterm, mostly since it's already been implemented. 

Unfortunately, I was not able to get this algorithm to converge in any reasonable amount of time when not using the quasi-newton enhancement and with the provided starting point (I remember it being said that this problem is very sensitive to the choice of starting point). Thus, instead, I found a starting point that works, and ran the unenhanced algorithm from that starting point. The point in question is 
$[0.4789,3.0474,0.5442,0.0146,-1.6093,0.5419,0.0514,0.2381,1.5101]$
and the other parameters were $\h{\Delta}=1$, $\eta=0.2$, $c=0.5$, and the stopping criterion was $\|Jr\|_2<\ep=10^{-3}$. This converged in 
\ssn{b}
For this part, the quasi-newton enhancement did make the algorithm converge faster than plain LM for the above starting point, but it was difficult to see any evidence of superlinear convergence there. However, starting from the point provided on chalk with other parameters the same as above except $\ep=10^{-5}$, the plot of ratios of successive differences looks like

\includegraphics[width=0.5\textwidth]{final_files/p1/1b_superlinear.png}

which does indicate that there is evidence of superlinear convergence.
\ssn{c}
Here is the plot of the fit superimposed over the data:

\includegraphics[width=\textwidth]{final_files/p1/1c_fit_data.png}

It's kinda hard to tell if there's a good fit, but the annual pattern seems to fit fairly well, and if I squint hard enough, I can sorta see the longer-term trends lining up. Here are the residuals:

\includegraphics[width=\textwidth]{final_files/p1/1c_resid_index.png}

Not much of a discernable pattern here. This suggests that the model probably captured all the information there was in the data. The best-guesses for the periods of the other two cycles are the reciprocals of $\h{\beta}_4$ and $\h{\beta}_7$, which are $44.3$ months and $26.9$ months, respectively. (this is starting from the point given on chalk).
\subsection*{2}
In the CG iteration, the remaining inequality constraints were dealt with by simply stopping the iteration and returning when the current point strayed beyond one of the bounds. Execution times for $n$ up to 100 are below. After running through the profiler, it turns out that most of the time was spent in the hot loop in \verb|get_active_bnds|, which is kinda unfortunate because otherwise I'd be able to work on much bigger values of $n$. Stopping conditions simply amount to checking that the inactive coordinates of the gradient are zero or close to so.

For speeding up the iteration, I decided to use the ideal preconditioner ($Z^TGZ$) because $Z$ is so simple that this product is as easy as slicing $G$, and $G$ is so sparse that doing the solve isn't very expensive. It also seems to have the added benefit of not straying outside the inequality bounds so that the CG loop gets to a point way closer to the optimal point, which in turn reduces the number of gradient projections, which are the thing that takes up the most time.

\begin{tabular}{c|cccccc}
    $n$&4&8&16&32&64&100\\
    \hline
    No preconditioner&0.2568&0.0166&0.0414&0.1984&1.7779&7.8181\\
    \hline
    Preconditioned&0.0020&0.0029&0.0080&0.0407&0.4883&2.0842\\
\end{tabular}

Note that this algorithm produces a monotone increase in the size of the active set of constraints -- once the gradient projection finds a local minimum that's constrained on an additional coordinate, the subsequent CG iteration will keep it within that constrained surface, and subsequent starting points will also be located on that constrained surface. Thus, once we get to the correct set of active constraints, the CG iteration should be able to hone in on the optimal point. 

Convergence, then, depends on the gradient projection eventually getting the active set right. For the sake of simplicity, suppose that there is no additional CG step and that subsequent projection steps continue where the last one found a local min. Then, in order to not end up with the correct active set, we'd have to get stuck on a too-small active set, which means that a local min is consistently found before the first breakpoint. However, this looks a lot like steepest descent line search, which we know to converge.

Finally, here's a picture with $n=200$ done by the preconditioned version:

\includegraphics[width=\textwidth]{final_files/p2/obstacle_200.png}
\subsection*{3}
For this problem, I'm choosing to use the interior point method outlined in section 16.6 and solving the problem in its primal form. First, in the primal form, the problem is particularly simple, with $G$ being the identity and $b$ zero. Second, it seems like a more direct approach than the augmented Lagrangian method, as the latter involves an extra layer of iteration (granted, with preconditioning in place, solving the box-bounded problem doesn't take many repeats, but it's still extra work). 

Reformulating the problem in standard constrained QP format, let $r=\openm w\\b\closem\in\rn^{m+1}$, with $w\in\rn^m$ and $b\in\rn$. Then, the problem becomes 
\begin{align*}
    \text{Minimize }f(r)&=\frac{1}{2}r^TGr,\ \text{where }G=\openm I&0\\0&0\closem\\
    \text{subject to }Ar&\geq\openm 1\\\vdots\\1\closem\\
    \text{where }A&=\openm y_1x_1^T&y_1\\y_2x_2^T&y_2\\\vdots&\vdots\\y_nx_n^T&y_n\closem\\
\end{align*}
By the form of $A$, it seems that this will be a fairly dense problem, as the provided $x_i$ are composed of zeros and ones with a good proportion of ones. 

My intent is to follow the framework in algorithm (16.4) in the textbook, with equal step lengths and modification of the provided starting point. The work then is in determining how to solves the systems to obtain the affine scaling step and the actual step. 

In this problem, we have that $m=123$, so that even if we take $n=1000$, we have a $1123$ by $1123$ dense matrix on our hands if we decide to go the route of solving (16.61). This seems like a fairly managably operation, seeing as an LDL factorization on matrices around this size take less than a second. However, larger values of $n$ might not do so well, as taking $n=30000$ would be pushing the limits on the size of machine memory. In that case, a projected CG approach like the book suggests might do better.

Here, though, I'm just going to go the route of the direct solution. To solve for the affine scaling step, we solve the system
\[\openm G&A^T\\A&-\Lambda^{-1}\mathscr{Y}\closem\openm\Delta x\\-\Delta\lambda\closem=\openm -Gx+A^T\lambda\\-Ax+e\closem\]
where $e$ is the $n$-vector of all $1$s. To solve for the actual step, transform (16.67) to the a form similar above to obtain
\[\openm G&A^T\\A&-\Lambda^{-1}\mathscr{Y}\closem\openm\Delta x\\-\Delta\lambda\closem=\openm-Gx-A^T\lambda\\-\Lambda^{-1}\Delta\Lambda^\text{aff}\Delta y^\text{aff}+\sigma\mu\Lambda^{-1}e-Ax+e\closem\]
Due to the diagonal nature of all of these matrices except for $A$, the code will not contain explicit matrix-vector multiplications for the most part.

As for stopping conditions, the gradient of the Lagrangian at some $r$ is $Gr - \lambda^T A$. 
\end{document}
