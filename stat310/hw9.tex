\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{1.5cm}
\subsection*{1}
\ssn{a}
By analogy with the standard preconditioning scheme for CG, we want to transform the problem by some matrix $D_k$ such that $\h{B_k}D_k^{-T}B_kD_k^{-1}$ has a nicer eigenvalue distribution that $B_k$ itself. The book states that this is equivalent to solving the subproblem 
\[\min_{\h{p}} f_k+\h{g}_k^T\h{p}+\frac{1}{2}\h{p}^T\h{B_k}\h{p}\]
where the true solution $p=D^{-1}\h{p}$ and $\h{g}_k=D^{-T}\nabla f_k$. 

Just like with ordinary CG, we can make the substitutions $\h{d}_i=D^{-1}d_i$, $\h{r}_i=D^{-T}r_i$, and $\h{z}_i=D^{-1}z_i$ to algorithm (7.2) to transform it into the preconditioned version. Prior to these substitutions, feeding the preconditioned minimization into the algorithm gives the initial state $z_0=0$, $r_0=D^{-T}\nabla f_k$, and $d_0=-D^{-T}\nabla f_k$. After the substitutions, we get $\h{z}_0=0$, $\h{r}_0=\nabla f_k$, and $\h{d}_0=D^{-1}D^{-T}\nabla f_k$. 

The main loop becomes the following, with the breaking conditions removed (I'll talk about them later)
\begin{align*}
    \alpha_i&=\frac{\h{r}_i^Ty_i}{\h{d}_i^TB\h{d}_i}\\
    \h{z}_{i+1}&=\h{z}_i+\alpha_i\h{d}_i\\
    \h{r}_{i+1}&=\h{r}_i+\alpha_iB\h{d}_i\\
    y_{i+1}&=D^{-1}D^{-T}\h{r}_{i+1}\\
    \beta_{i+1}&=\frac{\h{r}_{i+1}^Ty_{i+1}}{\h{r}_i^Ty_i}\\
    \h{d}_{i+1}&=-D^{-1}D^{-T}r_{i+1}+\beta_{i+1}\h{d}_i=-y_{i+1}+\beta_{i+1}\h{d}_i\\
\end{align*}

For the first breaking condition ($d_i^TBd_i\leq0$), we just need to convert it to $\h{d}_iBd_i\leq0$ by what we substituted for each of those things. In order to calculate $\tau$ and the return value, we transform back to the un-preconditioned problem. The algorithm that operates on $D^{-T}BD^{-1}$ returns $Dp$, so we have that $Dp=z_i+\tau d_i\implies p=\h{z}_i+\tau\h{d}_i$. Thus, we want to compute $\tau$ such that $\|p\|=\Delta_k$ and minimizes the approximation function (the original one).

The second and third breaking conditions work similarly, with the hatted version of the variables.
\ssn{b}
Suppose we have that $P^TAP=LDL^T$ for some $A$, where $D$ is composed of block diagonal elements of size at most $2$. If we let $\Lambda$ be a block diagonal matrix with the same structure as $D$. If the $i$th block of $D$ is size 1 and positive ($>\ep$), then let the $i$th block of $\Lambda$ be its square root. If the $i$th block of $D$ is size 2 and positive definite, the let the $i$th block of $\Lambda$ be the its Cholesky factor (which should be easy to compute analytically). Otherwise, let the diagonal of $\Lambda$ be composed of 1s. This will send the positive eigenvalues of $D$ which are not bound into indefinite blocks to $1$, which reduces the number of distinct eigenvalues.

Now, we have that there is some matrix $R$ with ``better'' eigenvalues such that $R=\Lambda^{-1}D\Lambda^{-T}$, or $R=\Lambda^{-1}L^{-1}P^TAPL^{-T}\Lambda^{-T}$. Our preconditioning matrix $C$ (sorry about the notation change from above -- I called the preconditioner $D$ in part (a)) then satisfies $C^{-T}=\Lambda^{-1}L^{-1}P^T$, or $C=\Lambda^TL^TP^T$. Thus, obtaining $C^{-1}C^{-T}x=y$ means solving the system $PL\Lambda\Lambda^TL^TP^Ty=x$. 

I implement this by having the CG routine take a function which computes $M^{-1}x$ as an argument, and write a function that computes $\Lambda$, then wraps it in a closure that computes $M^{-1}x$ through a series of solves and returns that. This avoids any matrix-matrix multiplication at all.
\ssn{c}
Running it on sizes $\{100, 200, 400, 600, 1000, 2000, 3000\}$ with random starting points, $\h{\Delta}=10$, $\eta=0.2$, and $\ep=10^{-7}$, we have the following results for timing and average number of CG iterations:

Non-preconditioned:

\begin{tabular}{c|ccccccc}
    $n$&100&200&400&600&1000&2000&3000\\
    Time&0.0230&0.0297&0.0304&0.0315&0.0383&0.0555&0.1077\\
    CG iters&16.8663&18.8310&16.6214&16.9496&18.1482&17.2296&16.6980\\
\end{tabular}

Preconditioned:

\begin{tabular}{c|ccccccc}
    $n$&100&200&400&600&1000&2000&3000\\
    Time&0.0409&0.0789&0.1626&0.2547&0.5128&1.3180&2.2316\\
    CG iters&1.0000&1.0000&1.0000&1.0175&1.0000&1.0000&1.0000\\
\end{tabular}

Looks like the preconditioning was a bit too powerful here. The CG loop finished in one iteration almost always, and the time shot way up, probably because there was a hot loop over $n$ in Matlab and some fancy stuff with closures. However, the un-preconditioned results already look pretty good -- 16 iterations for a Hessian with $n=3000$ makes me think that the Rosenbrock Hessian already has a pretty good spectrum for doing CG.
\ssn{d}
Below are two graphs showing the fractional decrease in the iterates, preconditioned on the left, unpreconditioned on the right, starting from a random point for $n=3000$.

\includegraphics[width=0.5\textwidth]{hw9_files/pc_superlinear.png}
\includegraphics[width=0.5\textwidth]{hw9_files/npc_superlinear.png}

There is pretty good indication of superlinear convergence, and it shows up even more if we start from all negative ones:

\includegraphics[width=0.5\textwidth]{hw9_files/ones_sl.png}

\subsection*{2}
Let $d$ be an arbitrary point in $\mathscr{F}(x^*)$. We want to show that $d$ is also in the tangent cone.

First, if we replace $\mathscr{A}(x^*)$ in the argument in the book with the matrix of equality constraint gradients $\mathscr{E}(x^*)$, and follow the argument until (12.42b), we obtain a feasible sequence of $z_k$ defined via the implicit function theorem by $z(t)$ as a solution to (12.40). This is possible because the MFCQ condition specifies that the equality constraint gradients are linearly independent. 

In addition, since the function and its constraints are smooth, the implicit function theorem also gives us an expression for $z'(0)$, which is the inverse Jacobian of $R$ at $t=0$ multiplied by the derivative of $R$ by $t$ at $t=0$. In other words, 
\[z'(0)=-\openm \mathscr{E}(x^*)\\Z^T\closem^{-1}\openm-\mathscr{E}(x^*)d\\-Z^Td\closem\]
Clearly, $d$ is a solution to this system. Since the Jacobian is invertible, it is the unique solution, so we have that $z'(0)=d$.

Now, let $w$ be some vector from the MFCQ conditions where $\mathscr{A}(x^*)w$ is positive in all its entries. For any $\ep>0$, we have that $d+\ep w\in\mathscr{F}(x^*)$, since $\ep w\in\mathscr{F}(x^*)$ by the form of the MFCQ conditions and the sum of two vectors in a cone is still in the cone. Then, there exists a curve $z_\ep(t)$ which is feasible for small $t$, with 
\[z_\ep'(0)=d+\ep w=\lim_{t\to 0}\frac{z_\ep(t)-z_\ep(0)}{t}\]
Note that $z_\ep(0)=x^*$ by the construction in the book, so this means that $d+\ep w$ is actually in the tangent cone for all $\ep>0$, which means that $d$ is a limit point of the tangent cone. Since the tangent cone is closed, $d$ is in the tangent cone.

For the particular problem, the $c_j$ are functions $z-\cos(2\pi j/n)x-\sin(2\pi j/n)y$ for $n\geq5$. The gradient of this is the vector $\openm -\cos(2\pi j/n)&-\sin(2\pi j/n)&1\closem$. For any $x$, if we take $w$ to be the vector with a $1$ in its third entry and zero elsewhere, then we satisfy the requirement that $\nabla c_j(x)^Tw=1>0$. Since there are no equality constraints, this means that the MFCQ conditions are satisfied, and the tangent cone and the feasible set are equal for any $x$ for this function.
\subsection*{3}
\ssn{a}
The gradient of the first constraint is $\openm-3(1-x_1)^2&-1\closem$ (evaluating to $\openm -3&-1\closem$ at $x^*$), and the gradient of the second constraint is $\openm0.5x_1&1\closem$ (evaluating to $\openm 0&1\closem$ at $x^*$). These two are linearly independent, so LICQ is satisfied.
\ssn{b}
The Lagrangian of this problem is $-2x_1+x_2-\lambda_1((1-x_1)^3-x_2)-\lambda_2(x_2+0.25x_1^2-1)$. Its gradient is
\[\openm-2+3\lambda_1(1-x_1)^2-0.5\lambda_2x_1\\1+\lambda_1-\lambda_2\closem\]
Plugging in $x^*$ gives $\openm 3\lambda_1-2\\1+\lambda_1-\lambda_2\closem$, so if we take $\lambda^*=\openm2/3\\5/3\closem$, the Lagrangian will be zero. Since we know that all the constraints are zero at $x^*$ and both $\lambda$s are positive, the KKT conditions are satisfied.
\ssn{c}
The set $\mathscr{F}(x^*)$ is the set of all points $d$ for which $d^T\openm-3\\-1\closem\geq0$ and $d^T\openm0\\1\closem\geq0$. This is a narrow cone delimited by the negative $y$-axis and the ray defined by $(1,-3)$. 

The set $\mathscr{C}(x^*, \lambda^*)$ is the union of the two half-lines generated by $(0, -1)$ and $(1, -3)$, since both components of $\lambda^*$ are positive.
\ssn{d}
The Hessian of the Lagrangian is the matrix
\[\openm -6\lambda_1(1-x_1)&-0.5x_2\\0&0\closem=\openm-4&-0.5\\0&0\closem\]
at $x^*, \lambda^*$. 
If we take $w=(0\ -1)^T$, then the value in question ($w^THw$) is zero. If we take $w=(1\ -3)^T$, then the value in question is also zero. Thus, the necessary conditions are satisfied, but the sufficient ones are not.
\subsection*{4}
Let $B$ be the block matrix $\openm A&I\closem$, where $I$ is the $n\times n$ identity. Then, let $K$ be the cone $\{Bx|x\geq0\}$. By Farkas', either $b\in K$ xor there exists some $y$ with $b^Ty<0$ and $B^Ty\geq0$. First, if $b\in K$, then there exists some $x=\openm x_1\\x_2\closem\geq0$ with $b=Bx=Ax_1+x_2$. Since $x_2\geq0$, this means that $b\geq Ax_1$ with $x_1\geq0$, so the first condition we have is implied. Conversely, if we suppose that there is some $x_1$ with $b\geq Ax_1$ and $x_1\geq0$, then let $x=\openm x_1\\b-Ax_1\closem\geq0$. Then $Bx=Ax_1+b-Ax_1=b$, so $b\in K$. 

For the second possibility, $B^Ty\geq0$ is equivalent to $\openm A^Ty\\y\closem\geq0$, which is in turn equivalent to $A^Ty\geq0$ and $y\geq0$. Along with $b^Ty<0$, this is equivalent to the second condition. Thus, since the two conditions we have correspond exactly to the mutual exclusion specified by Farkas', then these two conditions are mutually exclusive and one must always hold.
\end{document}
