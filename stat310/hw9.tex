\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{1.5cm}
\subsection*{1}
\ssn{a}
By analogy with the standard preconditioning scheme for CG, we want to transform the problem by some matrix $D_k$ such that $\h{B_k}D_k^{-T}B_kD_k^{-1}$ has a nicer eigenvalue distribution that $B_k$ itself. The book states that this is equivalent to solving the subproblem 
\[\min_{\h{p}} f_k+\h{g}_k^T\h{p}+\frac{1}{2}\h{p}^T\h{B_k}\h{p}\]
where the true solution $p=D^{-1}\h{p}$ and $\h{g}_k=D^{-T}\nabla f_k$. 

Just like with ordinary CG, we can make the substitutions $\h{d}_i=D^{-1}d_i$, $\h{r}_i=D^{-T}r_i$, and $\h{z}_i=D^{-1}z_i$ to algorithm (7.2) to transform it into the preconditioned version. Prior to these substitutions, feeding the preconditioned minimization into the algorithm gives the initial state $z_0=0$, $r_0=D^{-T}\nabla f_k$, and $d_0=-D^{-T}\nabla f_k$. After the substitutions, we get $\h{z}_0=0$, $\h{r}_0=\nabla f_k$, and $\h{d}_0=D^{-1}D^{-T}\nabla f_k$. 

The main loop becomes the following, with the breaking conditions removed (I'll talk about them later)
\begin{align*}
    \alpha_i&=\frac{\h{r}_i^Ty_i}{\h{d}_i^TB\h{d}_i}\\
    \h{z}_{i+1}&=\h{z}_i+\alpha_i\h{d}_i\\
    \h{r}_{i+1}&=\h{r}_i+\alpha_iB\h{d}_i\\
    y_{i+1}&=D^{-1}D^{-T}\h{r}_{i+1}\\
    \beta_{i+1}&=\frac{\h{r}_{i+1}^Ty_{i+1}}{\h{r}_i^Ty_i}\\
    \h{d}_{i+1}&=-D^{-1}D^{-T}r_{i+1}+\beta_{i+1}\h{d}_i=-y_{i+1}+\beta_{i+1}\h{d}_i\\
\end{align*}

For the first breaking condition ($d_i^TBd_i\leq0$), we just need to convert it to $\h{d}_iBd_i\leq0$ by what we substituted for each of those things. In order to calculate $\tau$ and the return value, we transform back to the un-preconditioned problem. The algorithm that operates on $D^{-T}BD^{-1}$ returns $Dp$, so we have that $Dp=z_i+\tau d_i\implies p=\h{z}_i+\tau\h{d}_i$. Thus, we want to compute $\tau$ such that $\|p\|=\Delta_k$ and minimizes the approximation function (the original one).

The second and third breaking conditions work similarly, with the hatted version of the variables.
\ssn{b}
Suppose we have that $A=LDL^T$ for some $A$, where $D$ is composed of block diagonal elements of size at most $2$. If we let $\Lambda$ be a block diagonal matrix with the same structure as $D$. If the $i$th block of $D$ is size 1 and positive ($>\ep$), then let the $i$th block of $\Lambda$ be its square root. If the $i$th block of $D$ is size 2 and positive definite, the let the $i$th block of $\Lambda$ be the its Cholesky factor (which should be easy to compute analytically). Otherwise, let the diagonal of $\Lambda$ be composed of 1s. 

Then, if we precondition with $\Lambda^{-1} L^{-1}AL^{-T}\Lambda^{-T}=\Lambda^{-1} D\Lambda^{-T}$, we manage to send all the positive eigenvalues of $D$ that aren't bound up in a indefinite block to $1$, which significantly reduces the number of distinct eigenvalues. In the preconditioning scheme, then, we'd have that $M^{-1}=\Lambda^{-1} L^{-1}L^{-T}\Lambda^{-T}$. If we include the permutation matrices into consideration, then $M^{-1}=\Lambda^{-1} L^{-1}P^TPL^{-T}\Lambda^{-T}$, which tells us that the permutation doesn't matter here.

I implement this by having the CG routine take a function which computes $M^{-1}x$ as an argument, and write a function that computes $\Lambda$, then wraps it in a closure that computes $M^{-1}x$ through a series of solves and returns that. This avoids any matrix-matrix multiplication at all.
\subsection*{2}
Let $d$ be an arbitrary point in $\mathscr{F}(x^*)$. We want to show that $d$ is also in the tangent cone.

First, if we replace $\mathscr{A}(x^*)$ in the argument in the book with the matrix of equality constraint gradients $\mathscr{E}(x^*)$, and follow the argument until (12.42b), we obtain a feasible sequence of $z_k$ defined via the implicit function theorem by $z(t)$ as a solution to (12.40). This is possible because the MFCQ condition specifies that the equality constraint gradients are linearly independent. 

In addition, since the function and its constraints are smooth, the implicit function theorem also gives us an expression for $z'(0)$, which is the inverse Jacobian of $R$ at $t=0$ multiplied by the derivative of $R$ by $t$ at $t=0$. In other words, 
\[z'(0)=-\openm \mathscr{E}(x^*)\\Z^T\closem^{-1}\openm-\mathscr{E}(x^*)d\\-Z^Td\closem\]
Clearly, $d$ is a solution to this system. Since the Jacobian is invertible, it is the unique solution, so we have that $z'(0)=d$.

Now, let $w$ be some vector from the MFCQ conditions where $\mathscr{A}(x^*)w$ is positive in all its entries. For any $\ep>0$, we have that $d+\ep w\in\mathscr{F}(x^*)$, since $\ep w\in\mathscr{F}(x^*)$ by the form of the MFCQ conditions and the sum of two vectors in a cone is still in the cone. Then, there exists a curve $z_\ep(t)$ which is feasible for small $t$, with 
\[z_\ep'(0)=d+\ep w=\lim_{t\to 0}\frac{z_\ep(t)-z_\ep(0)}{t}\]
Note that $z_\ep(0)=x^*$ by the construction in the book, so this means that $d+\ep w$ is actually in the tangent cone for all $\ep>0$, which means that $d$ is a limit point of the tangent cone. Since the tangent cone is closed, $d$ is in the tangent cone.

For the particular problem, the $c_j$ are functions $z-\cos(2\pi j/n)x-\sin(2\pi j/n)y$ for $n\geq5$. The gradient of this is the vector $\openm -\cos(2\pi j/n)&-\sin(2\pi j/n)&1\closem$. For any $x$, if we take $w$ to be the vector with a $1$ in its third entry and zero elsewhere, then we satisfy the requirement that $\nabla c_j(x)^Tw=1>0$. Since there are no equality constraints, this means that the MFCQ conditions are satisfied, and the tangent cone and the feasible set are equal for any $x$ for this function.
\subsection*{3}
\ssn{a}
The gradient of the first constraint is $\openm-3(1-x_1)^2&-1\closem$ (evaluating to $\openm -3&-1\closem$ at $x^*$), and the gradient of the second constraint is $\openm0.5x_1&1\closem$ (evaluating to $\openm 0&1\closem$ at $x^*$). These two are linearly independent, so LICQ is satisfied.
\ssn{b}
The Lagrangian of this problem is $-2x_1+x_2-\lambda_1((1-x_1)^3-x_2)-\lambda_2(x_2+0.25x_1^2-1)$. Its gradient is
\[\openm-2+3\lambda_1(1-x_1)^2-0.5\lambda_2x_1\\1+\lambda_1-\lambda_2\closem\]
Plugging in $x^*$ gives $\openm 3\lambda_1-2\\1+\lambda_1-\lambda_2\closem$, so if we take $\lambda^*=\openm2/3\\5/3\closem$, the Lagrangian will be zero. Since we know that all the constraints are zero at $x^*$ and both $\lambda$s are positive, the KKT conditions are satisfied.
\ssn{c}
The set $\mathscr{F}(x^*)$ is the set of all points $d$ for which $d^T\openm-3\\-1\closem\geq0$ and $d^T\openm0\\1\closem\geq0$. This is a narrow cone delimited by the negative $y$-axis and the ray defined by $(1,-3)$. 

The set $\mathscr{C}(x^*, \lambda^*)$ is the union of the two half-lines generated by $(0, -1)$ and $(1, -3)$, since both components of $\lambda^*$ are positive.
\ssn{d}
The Hessian of the Lagrangian is the matrix
\[\openm -6\lambda_1(1-x_1)&-0.5x_2\\0&0\closem=\openm-4&-0.5\\0&0\closem\]
at $x^*, \lambda^*$. 
If we take $w=(0\ -1)^T$, then the value in question ($w^THw$) is zero. If we take $w=(1\ -3)^T$, then the value in question is also zero. Thus, the necessary conditions are satisfied, but the sufficient ones are not.
\subsection*{4}
Let $B$ be the block matrix $\openm A&I\closem$, where $I$ is the $n\times n$ identity. Then, let $K$ be the cone $\{Bx|x\geq0\}$. By Farkas', either $b\in K$ xor there exists some $y$ with $b^Ty<0$ and $B^Ty\geq0$. First, if $b\in K$, then there exists some $x=\openm x_1\\x_2\closem\geq0$ with $b=Bx=Ax_1+x_2$. Since $x_2\geq0$, this means that $b\geq Ax_1$ with $x_1\geq0$, so the first condition we have is implied. Conversely, if we suppose that there is some $x_1$ with $b\geq Ax_1$ and $x_1\geq0$, then let $x=\openm x_1\\b-Ax_1\closem\geq0$. Then $Bx=Ax_1+b-Ax_1=b$, so $b\in K$. 

For the second possibility, $B^Ty\geq0$ is equivalent to $\openm A^Ty\\y\closem\geq0$, which is in turn equivalent to $A^Ty\geq0$ and $y\geq0$. Along with $b^Ty<0$, this is equivalent to the second condition. Thus, since the two conditions we have correspond exactly to the mutual exclusion specified by Farkas', then these two conditions are mutually exclusive and one must always hold.
\end{document}
