\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\newcommand{\zn}{\mathbb{Z}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{20pt}

\section*{Changes to original design}
\subsection*{Program structure}
Due to discussion in class on the use of barriers, the original idea of creating new threads upon each iteration of the outer loop was scrapped. Instead, $T$ threads are created at the start of execution, and they live for the entire duration of the algorithm. Consequently, the structure that is passed to the worker threads now includes a pointer to the barrier object. In addition, the program no longer considers empty graphs/null pointers to be valid input to the \verb|fw_parallel| and \verb|fw_serial| functions.

In addition to the C code that performs the numerical work, several functions located in \verb|wrapper.c| were written to provide an interface to Python for testing purposes. The Python functions were named \verb|fw_parallel| and \verb|fw_serial| in the \verb|wrapper| package, and their arguments include a NumPy integer array (for the adjacency matrix), the number of nodes, and the number of threads in the parallel case. They both return a NumPy integer array filled with shortest-path values. There are also two functions to load and dump csv files for the matrices that take a filename, the matrix, and the number of nodes.
\subsection*{Test plan}
Correctness tests were implemented in Python with the \verb|unittest| module mostly according to the original plan. For the tests that specified $10$, $50$, or $100$ nodes, an additional size of $500$ was added because it ran a lot faster than I expected. The tests that involved one undirected edge were omitted since I felt that they were redundant with the one directed edge tests. The highest thread count for the correctness tests was also bumped up to $32$. 

Performance tests were conducted according to the original plan for parallel overhead and speedups. For parallel overhead and speedups, each $(N,T)$ combination was run $9$ different times with different matrices so as to reduce the effect of random fluctuations. The mean and standard deviation were reported. 

For time spent waiting, the original plan was to time the difference between when the first thread returned and when the last thread returned. Since this became impossible with the use of the barrier, a new system was put into place. A timer was run for each thread to time each iteration of the outer loop, and a separate timer was used to time the time spent in the call to \verb|pthread_barrier_wait|. The times of these were stored in two arrays in the thread data structure which were then processed by the main thread after all threads have finished. Three values were reported: the ratio of the average wait time to the average outer-loop time, the average outer-loop time, and the standard deviation of the outer-loop times. 
\section*{Hypotheses}
For the parallel overhead, the original hypothesis was that the difference in runtimes between the parallel and serial versions with $T=1$ would increase linearly. Since this was not the correct definition of parallel overhead, we instead recorded the ratio between the serial runtime and parallel runtime. Translated to this parameter, the original hypothesis becomes that the ratio will approach $1$ as $N$ increases. The results of that are presented below in a graph. The error bars denote the standard deviation of the $9$ measurements taken for each $N$. The $x$-axis has the number of nodes, and the $y$-axis has the ratios.

\includegraphics[width=0.6\pagewidth]{img/overhead.png}

The trend does indeed seem to be trending upwards, though there is a leveling-off effect towards the tail. The point at $128$ nodes is a bit of a departure from the trend, but its high standard deviation suggests that there could be a low outlier pulling at the point. Beyond that, I am unsure of what could be causing that dip. In addition, the trend seems to be asymptotically approching $0.7$ rather than $1$, which suggests that there is some sort of $O(n^3)$ slowdown in the parallel version. This could be because of the different way memory is accessed in the parallel version -- there is a good deal of index computation going on in the inner loop.

As for the speedups with multiple threads, the information is in the following graph. The $x$-axis contains the number of threads, while the $y$-axis has the speedup.

\includegraphics[width=0.6\pagewidth]{img/speedups.png}

As expected, the speedups generally increase with increasing number of nodes, but the trend is decidedly nonlinear in the number of threads. The machine we're running this on has $16$ cores, and this can be seen in the big peak for $N=1024$ at $16$ threads. However, for smaller $N$, the peak comes for smaller values of $T$. This is probably due to the increased overhead experienced by the parallel algorithm for larger values of $T$ (as seen below). 


