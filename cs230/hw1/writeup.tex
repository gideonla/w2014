\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\newcommand{\zn}{\mathbb{Z}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{20pt}

\section*{Changes to original design}
\subsection*{Program structure}
Due to discussion in class on the use of barriers, the original idea of creating new threads upon each iteration of the outer loop was scrapped. Instead, $T$ threads are created at the start of execution, and they live for the entire duration of the algorithm. Consequently, the structure that is passed to the worker threads now includes a pointer to the barrier object. In addition, the program no longer considers empty graphs/null pointers to be valid input to the \verb|fw_parallel| and \verb|fw_serial| functions.

In addition to the C code that performs the numerical work, several functions located in \verb|wrapper.c| were written to provide an interface to Python for testing purposes. The Python functions were named \verb|fw_parallel| and \verb|fw_serial| in the \verb|wrapper| package, and their arguments include a NumPy integer array (for the adjacency matrix), the number of nodes, and the number of threads in the parallel case. They both return a NumPy integer array filled with shortest-path values. There are also two functions to load and dump csv files for the matrices that take a filename, the matrix, and the number of nodes.
\subsection*{Test plan}
Correctness tests were implemented in Python with the \verb|unittest| module mostly according to the original plan. For the tests that specified $10$, $50$, or $100$ nodes, an additional size of $500$ was added because it ran a lot faster than I expected. The tests that involved one undirected edge were omitted since I felt that they were redundant with the one directed edge tests. The highest thread count for the correctness tests was also bumped up to $32$. 

Performance tests were conducted according to the original plan for parallel overhead and speedups. For parallel overhead and speedups, each $(N,T)$ combination was run $9$ different times with different matrices so as to reduce the effect of random fluctuations. The mean and standard deviation were reported. 

For time spent waiting, the original plan was to time the difference between when the first thread returned and when the last thread returned. Since this became impossible with the use of the barrier, a new system was put into place. A timer was run for each thread to time each iteration of the outer loop, and a separate timer was used to time the time spent in the call to \verb|pthread_barrier_wait|. The times of these were stored in two arrays in the thread data structure which were then processed by the main thread after all threads have finished. Three values were reported: the ratio of the average wait time to the average outer-loop time, the average outer-loop time, and the standard deviation of the outer-loop times. 
\section*{Hypotheses}
For the parallel overhead, the original hypothesis was that the difference in runtimes between the parallel and serial versions with $T=1$ would increase linearly. Since this was not the correct definition of parallel overhead, we instead recorded the ratio between the serial runtime and parallel runtime. Translated to this parameter, the original hypothesis becomes that the ratio will approach $1$ as $N$ increases. The results of that are presented below in a graph. The error bars denote the standard deviation of the $9$ measurements taken for each $N$. The $x$-axis has the number of nodes, and the $y$-axis has the ratios.

\includegraphics[width=0.5\textwidth]{img/overheads.png}

The trend does indeed seem to be trending upwards, though there is a leveling-off effect towards the tail. The point at $128$ nodes is a bit of a departure from the trend, but its high standard deviation suggests that there could be a low outlier pulling at the point. Beyond that, I am unsure of what could be causing that dip. In addition, the trend seems to be asymptotically approching $0.7$ rather than $1$, which suggests that there is some sort of $O(n^3)$ slowdown in the parallel version. This could be because of the different way memory is accessed in the parallel version -- there is a good deal of index computation going on in the inner loop.

As for the speedups with multiple threads, the information is in the following graph. The $x$-axis contains the number of threads, while the $y$-axis has the speedup.

\includegraphics[width=0.6\textwidth]{img/speedups.png}

As expected, the speedups generally increase with increasing number of nodes, but the trend is decidedly nonlinear in the number of threads. The machine we're running this on has $16$ cores, and this can be seen in the big peak for $N=1024$ at $16$ threads. However, for smaller $N$, the peak comes for smaller values of $T$. This is probably due to the increased overhead experienced by the parallel algorithm for larger values of $T$ (as seen below). 

However, the speedup nowhere near the number of processors even for $T\leq16$, so the original hypothesis there was way off. This is probably due to the time spent waiting at the barrier at the end of each iteration of the outer loop.

Finally, the last original hypothesis was that each thread will take the same amount of time to execute the outer loop, and consequently, the ratio of time spent waiting between iterations to the time spend performing calculations will be low. This was measured here by taking the average (over iterations of the outer loop) maximum (over threads) time spent inside the \verb|pthread_barrier_wait| function and dividing it by the average time for one iteration of the outer loop. 
Admittedly, this is a rather problematic way of measuring things. The barrier function has unknown overhead which may depend on $T$ somehow, and the timers introduce a fair bit of overhead themselves. Nevertheless, some sensible data came out of it, which is summarized in the graph below.

\includegraphics[width=0.6\textwidth]{img/waits.png}

The $x$-axis is the number of threads, and the $y$-axis is the aforementioned ratio. As would be expected, this ratio is very low when $T=1$, as the barrier function should just return immediately every time. Also, the ratio is lower for larger $N$, which makes sense since the inner loop takes much longer. We also see that the ratio increases for increasing $T$: this is probably due to a combination of two factors. First, for a given $N$, increasing $T$ reduces the amount of work that the inner loop has to do, which will boost up the ratio. Second, increasing $T$ will probably also increase the amount of work that the barrier has to do, so that will contribute to increasing the ratio as well.
The ratio is also atrociously large for small values of $N$ and large values of $T$. In fact, for $T=1$ and $N=16$, the value is about $1/2$, which suggests that the barrier is responsible for a good amount of the parallel overhead.

To see how much the execution time of the inner loop varies, we can look at the standard deviations of the inner loop execution times. This is presented in the graph below:

\includegraphics[width=0.6\textwidth]{img/outerloop_data.png}

The $x$-axis is number of threads, the $y$-axis is time in milliseconds, and the error bars represent the standard deviation of the inner loop execution times. At least for the $N=512$ and $N=1024$ cases, the standard deviations seem to be decently large compared to the actual value, which suggests that the threads aren't all finishing at the same time. This indicates that we might want to consider some sort of adaptive work-assignment scheme, though that would involve a critical section in the code and more overheads that we will need to seek to minimize by tweaking the size of the chunks that get handed out.
\end{document}
