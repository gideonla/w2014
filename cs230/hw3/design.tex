\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\newcommand{\zn}{\mathbb{Z}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\begin{document}
Name: Hall Liu

Date: \today 
\vspace{1.5cm}

\section*{Structure}
\subsection*{Lock interface}
Due to the varied number of locks we're using, in order to be able to apply the locks in a consistent manner, all locks will be abstracted with the following struct:
\begin{verbatim}
struct lock_t {
    void (*lock)(lock_t *);
    void (*unlock)(lock_t *);
    bool (*try_lock)(lock_t *);
    void (*destroy)(lock_t *);
};
\end{verbatim}
Each distinct type of lock will have these four function pointers in the beginning of its data in this order, so we will be able to cast freely between \verb|lock_t| and the specific lock types. Each lock will have its own specifically-named creation function, and said creation function will put the appropriate function pointers into those slots. 

The calling functions should not have to manage any thread-local constructs for these locks -- pthreads provides some thread-local mechanisms (the pthreads keys) that can replace the functionality of the \verb|ThreadLocal| class in Java, and all that stuff should just be taken care of by the lock-specific functions. The idea is to make the lock as opaque as possible to the calling function. There was something posted on Piazza about how this mechanism was slow, but since we're only calling it once per invocation of lock, I think it's a good tradeoff for making the interface simpler.

We will also have a null lock that implements this interface, but actually does nothing when its \verb|lock| or \verb|unlock| functions are called -- this is to facilitate the implementation of the locked queue.

We also should specify that attempting to acquire a lock that the same thread is already holding will result in undefined behavior -- there is no reason why this should happen, and the correctness tests will not account for this.
\subsection*{Queue}
The queue object here will behave exactly the same as the one from before, with the exception of a \verb|lock| parameter to the queue creation function. This will be an integer corresponding to the type of lock we want to be associated with the queue, with the option for the null lock to create a lock-free queue. The lock will be called internally from \verb|deq|, so there will be no need to interact with the lock object from any caller of the queue functions. 

\verb|deq| will also give the caller the option to fail upon encountering a locked queue via an additional parameter -- the return code will be $2$ to indicate a lock failure as opposed to $1$, which indicates an empty queue.
\section*{Correctness tests}
\ssn{Locks}
Due to the unified interface we proposed above for the locks, we can run the same tests on them with the same code. First, the most important invariant that a lock is to follow is mutual exclusivity. To test this, we use the following scenarios with variying numbers of workers:
\begin{enumerate}
    \item Main thread creates and acquires a lock. It spawns some number of worker threads which all try to acquire the lock, while the main thread never releases it. If any of the worker threads enter a section after the lock is acquired, the test fails.
    \item Main thread allocates an array of zeros and initiates a counter, then spawns workers which increment the counter under a lock and flips the array element corresponding to the old value of the counter. At the end, we expect to see an array of all nonzero values, and any zero values indicate that mutual exclusivity was broken at some point. This tests mutual exclusivity under hectic conditions.
\end{enumerate}
Also, we want to ensure the correct behavior under lack of contention. We can have the main thread initiates a lock, then proceed to lock and unlock it in rapid succession for a large number of iterations. The thread should not hang at any point. 

Finally, we want to guarantee FIFO ordering for the queue locks. To do this, we initiate a lock, then spawn some number of worker threads, each assigned a number $k$ from $3$ to $3+n$. Each worker thread will sleep for $10\cdot k$ milliseconds, then attempt to acquire the lock. Once it has acquired the lock, it will wait $100$ms, then print its value of $k$. We expect that the values of $k$ are printed out in increasing order. 
\ssn{Load-balancing strategies}
In the random queue selection strategy, we can test for correctness by having each worker store the sequence of queues that it hits, processing this information from all workers, and test for uniformity of the hit distribution. This verifies that the load balancing strategy is behaving as expected. For the last-queue approach, we can set up a scenario with $n$ queues and only one worker, then fill up $k<n$ of these queues and start the worker. We will want to verify that the queues are emptied out in order, with the worker always finishing one queue before starting on any other.

Finally, to verify the overall correctness of the worker-queue-dispatcher system, we can check the fingerprint-sum against the plain serial version as we did in the last homework.
\section*{Performance hypotheses}
\subsection*{Counter tests}
\ssn{Idle overheads}
Here, we measure uncontested lock performance by comparing performance of counter-incrementing under no locks to counter-incrementing by grabbing an uncontested lock every time. There are two approaches -- the fixed-time approach and the fixed-work approach. Of these, if there is any appreciable difference, the fixed-time approach will show higher throughput. This is because the timer may allow the counter to run for longer than specified, and there is additional overhead in fixed-work with checking if the work limit has been reached every iteration. However, we do not expect a large difference here.

For the actual locks, we expect to see the highest performance with TAS. The uncontested path through the lock function is extremely low-overhead, being only a few cpu instructions. This should be closely followed or even equaled by the exponential backoff lock, as the uncontested path is about the same, with some possible overhead in backoff depending on the final implementation.

The other three locks will probably see pretty bad performance in comparison -- they all use some form of thread-local storage which is apparently slow (this is unverified, and will be one of the performance experiments). Of these, the array lock will probably be the fastest, as it doesn't involve as many memory operations as the list-based locks. Next is probably the CLH lock, since the MCS lock involves more memory operations.

Finally, the performance of the pthreads lock is unpredictable, since understanding it would probably involve a dive into the glibc and kernel source code. I hear that it's fairly slow though.
\ssn{Lock scaling}
The TAS lock is known to scale extremely poorly with increasing number of threads. The exponential backoff lock works quite a bit differently from the queue locks, so its performance relative to those is difficult to predict. Within the queue-based locks, we'd expect the array lock and the CLH lock to scale similarly, as their locking and unlocking functions should behanve fairly similar to each other when we change the number of threads -- the spinning behavior is the same, the FIFO ordering is preserved, and there's no cache sharing. Finally, the MCS lock is expected to scale not as well as the other two due to the spinning-on-unlock behavior -- when $n$ exceeds the number of cores, we may see a lot of hanging on unlocks due to the ``next'' thread being descheduled.

As for fairness, we'll probably see a large standard deviation on TAS and something even larger on exponential backoff due to their inherently random nature. The queue locks will probably have lower standard deviation, with the MCS lock having more variation due to the two spin cycles.
\subsection*{Packet tests}
\ssn{Lock overheads}
Since we're doing basically the same thing with the locks as the idle counter overheads, we should expect to see similar relative performance between the locks. The overall performance impact should be much lower, as the counter measurements did not have the thread do any appreciable work besides lock and unlock, whereas the workers are actually doing something here.
\ssn{The three load-balancing strategies}
The load-balancing approaches given in the assignment document are the non-balanced approach, random queue selection, and the ``last-queue'' strategy. In the non-load-balanced approach, we have two problems. If a worker is finishing the packets faster than the dispatcher provides them, it has no choice other than to idle and spin on the empty queue, and if a worker is backlogged on a packet, the queue corresponding to that worker will be full as long as that worker is backlogged. Poor performance on this approach is expected for the exponential case and less so for the uniform case, as the larger spread of workloads from exponential is more likely to cause empty queues or worker backlogs.

The random queue selection approach helps to address the idle worker problem -- unless most of the queues are empty, any individual worker will probably spin less than if it was just watching one empty queue. However, we do get a bit of overhead from the random number generation, and this may result in filled-up queues because the number of worker hits to any one given queue should tend to become normally distributed. Worse performance here with the uniform distribution relative to the non-balanced approach indicates that the full-queue problem is significant, and worse performance here with the exponential distribution indicates that the full-queue problem is actually worse than the idle-worker problem. Note that the problem of lock contention probably isn't significant here -- as long as the workload is big enough, it is very unlikely that a large number of workers will be trying to dequeue from the same queue at the same time.

The ``last-queue'' strategy is fairly similar, with the exception that the worker will not wait for a particular queue to be unlocked, but rather hop around until it finds one. In addition, it will commit to finishing that queue before it hops on to another randomly chosen one. This strategy preserves the no-idling-workers property of the previous rule, and it will also serve to empty out filled queues quicker. However, it seems that this will tend to exacerbate the problem of full queues -- two or more workers could easily home in on the same queue, and this would leave other queues to just accumulate packets because there are the same number of workers and queues.

It seems that the ``last-queue'' strategy will have poorer performance than the random selection strategy. The doubled-up worker issue is my biggest concern here. If this occurs, then the queue will be emptied fairly quickly, but there will be lock contention on that queue, and at least one other queue will go unserved during that time period. However, without actually implementing the program and actually measuring performance, it is hard to tell.
\ssn{The influence of lock selection}
In the random queue approach, we expect to see less lock contention than in last-queue. Thus, the performance of the selected lock under contention will matter for whether it does better under random queue or under last-queue, which means that TAS and exponential backoff will probably perform better under random queue than under last-queue. Since we don't know for sure what the performance of the queue locks under contention looks like, the hypothesis for those will be reserved until the experiments for those have been run, but it should follow the pattern described.

All other factors kept the same, the behavior of the locks under varying numbers of $n$ should follow the results we got from lock scaling in the counter tests. 
\ssn{The ``awesome'' load-balancing strategy}
In order to design a better approach, we need to identify what is causing poor performance in the existing approaches and aim to rectify that in the new approach. Ideally, to achieve maximum throughput, we want each worker to be spending as much time in the \verb|getFingerprint| function as possible and as little time spinning on a queue as possible. In addition, we want the dispatcher to be skipping over full queues as little as possible and actually enqueueing as much as possible. However, it seems that the former will pose a much larger problem for speed. In the last assignment, we rarely saw the speedup flatline due to hitting up against the dispatcher rate, and the dispatcher skipping over a queue and going on is considerable less expensive than having a worker spin until the dispatcher gets around to them again (1 dispatcher iteration versus $n$ dispatcher iterations).

It is evident that there is a tradeoff between the two things we want. On one hand, if a queue is constantly being watched by one or more workers, then it is less likely to fill up but more likely that the workers will sit and spin. On the other hand, if the workers greedily go after work in whatever queue, this increases the risk that some queue will be overlooked and therefore get filled up. Then, on this spectrum, we can rank the existing strategies as follows. The non-balancing approach sits to the extreme of making sure that the queues are being watched. The last-queue strategy is at the extreme of having the workers actively go where the work is (stay at a queue because it's known to have work, leave only when it's empty). The random selection approach is a compromise between these two.

Thus, depending how the approaches actually perform, we can try and locate a good place for the new strategy in this spectrum. There are a sufficient number of possibilities that it would be pointless to come up with designs for each case, so I'll defer that to the writeup so that I only have to come up with one alternative design.
\end{document}
