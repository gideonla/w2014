\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\newcommand{\zn}{\mathbb{Z}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{20pt}

\section*{Changes to original design}
\subsection*{Program structure}
Due to discussion in class on the use of barriers, the original idea of creating new threads upon each iteration of the outer loop was scrapped. Instead, $T$ threads are created at the start of execution, and they live for the entire duration of the algorithm. Consequently, the structure that is passed to the worker threads now includes a pointer to the barrier object. In addition, the program no longer considers empty graphs/null pointers to be valid input to the \verb|fw_parallel| and \verb|fw_serial| functions.

In addition to the C code that performs the numerical work, several functions located in \verb|wrapper.c| were written to provide an interface to Python for testing purposes. The Python functions were named \verb|fw_parallel| and \verb|fw_serial| in the \verb|wrapper| package, and their arguments include a NumPy integer array (for the adjacency matrix), the number of nodes, and the number of threads in the parallel case. They both return a NumPy integer array filled with shortest-path values. There are also two functions to load and dump csv files for the matrices that take a filename, the matrix, and the number of nodes.
\subsection*{Test plan}
Correctness tests were implemented in Python with the \verb|unittest| module mostly according to the original plan. For the tests that specified $10$, $50$, or $100$ nodes, an additional size of $500$ was added because it ran a lot faster than I expected. The tests that involved one undirected edge were omitted since I felt that they were redundant with the one directed edge tests. The highest thread count for the correctness tests was also bumped up to $32$. 

Performance tests were conducted according to the original plan for parallel overhead and speedups. For parallel overhead and speedups, each $(N,T)$ combination was run $9$ different times with different matrices so as to reduce the effect of random fluctuations. The mean and standard deviation were reported. 

For time spent waiting, the original plan was to time the difference between when the first thread returned and when the last thread returned. Since this became impossible with the use of the barrier, a new system was put into place. A timer was run for each thread to time each iteration of the outer loop, and a separate timer was used to time the time spent in the call to \verb|pthread_barrier_wait|. The times of these were stored in two arrays in the thread data structure which were then processed by the main thread after all threads have finished. Three values were reported: the ratio of the average wait time to the average outer-loop time, the average outer-loop time, and the standard deviation of the outer-loop times. 
\section*{Results}
Unless otherwise specified, all experiments were conducted with 5 iterations at the same parameter averaged together and compiled with the \verb|-DPERF| flag.
\subsection*{Parallel overhead}
Here is a plot of speedup (serial-queue time divided by serial time), with speedup on the y-axis and workload on the x-axis:

\includegraphics[width=0.5\textwidth]{img/parallel_overheads.png}

We indeed do see a decreasing trend as the workload increases, and the dependence on the number of sources is very weak as expected. However, I am at somewhat of a loss as to why the speedup can drop below 1 (i.e. the serial queue version is faster than plain serial). Within the parts covered by the timer, the serial queue version does at least as much work as the plain serial version. 

If we calculate worker rate as described in the design document, regressing the reciprocal of packet rate against workload gives a slope of $m_W=2.618\times10^{-6}$. Then, to obtain the packet rate for any particular workload $W$, we just take $\frac{1}{W\cdot m_W}$.
\subsection*{Dispatcher rate}
The plot of packets per second versus number of sources follows, with number of sources on the $x$-axis.

\includegraphics[width=0.5\textwidth]{img/dispatcher_rate.png}

There is some wild variability here. The general trend is that packet rate starts low, goes up, then comes down again as $n$ increases. The initial increase might be explained by the fact that the worker still takes a non-negligible amount of time to do its thing, even for $W=1$. Then, if the queues somehow start filling up with $n=1$ or $n=2$, we'll see a hit to the dispatch rate. Also, the decrease at $n=31$ can be explained by scheduling -- there are more threads than processors, so some threads aren't going to execute for a while, potentially causing their queue to fill up.

Due to the variability, we cannot derive a single dispatch rate for all values of $n$, so we'll just use the numbers we have here.
\subsection*{Uniform speedup}

\end{document}
