\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\newcommand{\zn}{\mathbb{Z}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{20pt}

\section*{Changes to original design}
\subsection*{Program structure/interfaces}
The linked lists were not unified into one structure. Instead, the lock-free and serial lists were given their own interfaces with the same type signatures, except add and contains on lock-free, which now take an integer pointer as a place to store the number of steps taken.

The hash tables' destroy functions were removed, since for the purposes of this assignment, memory leaks are rampant anyway and the hash table is only destroyed at the end of the process, at which point the OS cleans up anyway.
\subsection*{Hash table implementation choices}
For the lock-free contains table implementation (henceforth referred to as ``lfc''), the original design of having resize copy the whole table was abandoned due to implementing preallocation for more parity in comparison to the fully lock-free table (henceforth referred to as the ``split'' table). Instead, the strategy was to copy all the nodes that need to be moved over to their new locations, then double the table capacity. Immediately after, a counter of the total contains access count was swapped out for a new one with zero value, then resize will wait for the old one to go to zero, at which point it removes all the old nodes from the table.

Due to the recursive split-ordering used in the list in the lfc table, dropping the old nodes simply consisted of remembering the last node that stays in place and swapping its next pointer for a null. I made this choice consciously because it significantly simplified the implementation of resize, but I do expect it to boost the speed of lfc somewhat because contains now never has to traverse the old nodes, even before resize has removed them.

For the linear-probe table, the old design ran into a problem when I discovered that I couldn't upgrade a read-lock to a write-lock when I find the element that I want to change. Thus, a new locking scheme was devised. The read-write lock on the whole table for resizing was maintained, but now each array node has two locks: a data lock to protect the key and the packet and a distance lock to protect the max distance. When add goes in, it grabs the distance lock and the data lock on the first node, but it only holds the distance lock as it walks along. This way, each thread holds at most one lock of each type at any moment, which eliminates the deadlock risk.
\section*{Summary of tests}
The tests changed significantly between the original design and present. All the tests are documented in the two files containing test functions, but for the sake of completeness, I'll paste them here along with the test cases used.
\subsection*{List tests}
First, the tests for the serial list. These were all tested over $N\in \{1,2,4,100\}$.

\begin{enumerate}
    \item Inserts N distinct random elements into a list, then tests that they're still all there. 
    \item Inserts N distinct random elements into the list, removes half of them, then tests that contains functions correctly.
    \item Inserts N random elements with keys $\leq2N$, then tests 20 random keys above 2N to make sure that they're not contained.
    \item Inserts a bunch of random elements in, and tests that the ordering makes sense.
    \item Calls contain and remove several times on an empty list. (no arguments)
\end{enumerate}

The tests for the lockfree list. All tests were tested with $N\in\{1,2,4,100,2000\}$, $T\in\{1,2,4,8,16\}$, where applicable.

\begin{enumerate}
    \item Adds in N distinct keys to the list using T threads, then uses T threads to test that they're in the list.
    \item Tests concurrent adds and contains -- Adds N random keys, then adds an additional distinct N while testing containment on the first N. All results should be successful. Assume T divisible by 2.
    \item Tests concurrent adds, contains, and removes. Add in 2N keys,then call adds on N more, removes on the second N, and contains on the first N. Everything should succeed. Proceed to test containment on all keys. Should succeed on first N, fail on second block, succeed on third block. This test uses three parameters, $T_a$, $T_r$, and $T_c$, corresponding to the number of threads used. See test.py for the values.
    \item Tests to make sure that indistinct adds will fail. Attempts to insert R distinct values a total of N times, and asserts that there are only R successes. Tested over $R$ in the same range as $T$.
    \item Does a parallel add of 2N elements, then a parallel removal of the first N and makes sure the order is correct.
    \item Finally, a test similar to that of the last test in the serial case.
\end{enumerate}
\subsection*{Hash table tests}
For the hash tables, the exact same tests were used as for the lock-free list except the last two, since they're both fundamentally implementations of sets. There were two other specific tests -- one which inserted $N$ elements into a lock-free-contains list, then spawned off $T-1$ workers to call contains repeatedly on these while the main thread called resize repeatedly. The expectation is that contains will always succeed. The other test is one to test that indices get initialized correctly in the split table implementation -- $T$ workers loop over $N$ indices and call initialize on them all, then verifies that the resulting linked list has exactly $N$ sentinal nodes, with each index pointing to the correct one.
\section*{Results}
Results were obtained with the average of $4$ iterations of the same test. The values of $n$ tested for the speedup were $1,2,3,7,15,31$, where $n$ is the number of workers.
\subsection*{Dispatcher rate}
As measured here, the dispatcher rate was 2650 packets/ms. This is somewhat above the dispatcher rates measured from hw2, which ranged from 1600 to 2400. The dispatcher from hw2 was significantly more complex than this one due to it using work-based measurements, which added a good deal of bookkeeping, so it makes sense that the dispatcher would be faster here.
\subsection*{Parallel overhead}
For this and the rest of the performance measurements, I used $W=500$ instead of $W=4000$. Comparatively, far more of the work is going to be done in the getFingerprint function rather than in the hash table functions, so reducing the workload has the effect of better discrimination between the performances of the different hash tables. In addition, I set the mysterious preload parameter to 128, so as to allow the tables to skip the large amounts of resizing near the start.

Now, here is a table of speedups for $n=1$, $W=500$ under the read configuration.

\begin{tabular}{c|cccc}
    Table&locked&lfc&probe&split\\
    \hline
         &$1.072$&$1.096$&$1.078$&$1.083$\\
\end{tabular}

A similar table for the write configuration:

\begin{tabular}{c|cccc}
    Table&locked&lfc&probe&split\\
    \hline
         &$0.918$&$0.886$&$0.892$&$0.894$\\
\end{tabular}

I didn't write the serial implementation, so I'm not quite sure why the speedup is greater than 1 for the read configuration. A quick glance through the provided code shows that the lists are not contiguous with each other, so perhaps the lack of locality is what caused the slowdown. Otherwise, the code path for contains seems to be the same. 

Unsurprisingly, the lfc and the split table had the least overhead for the read configuration. The split table does a bit more logic on contains to check the incremental resize condition and the initialization state of the bucket it hashed into, so it makes sense that it would be slower.

For the write configuration, there was actually overhead. The add functions in lfc and split both go through the add function for the lock-free list, which does a fair amount of work to make sure that the list remains in a consistent state. The locked table probably has the most similarity to the serial table in terms of code path, with the only overhead coming from the acquisition of the r/w lock. 
\subsection*{Preload effect}
The following two graphs plot the number of items preloaded versus throughput. The read config is on left, the write config on right. This was run on 15 sources + 1 dispatcher, with initial table capacity set to 16. As before, $W=500$.

\includegraphics[width=0.5\textwidth]{img/rd_preload.png}
\includegraphics[width=0.5\textwidth]{img/wr_preload.png}

For increasing values of $P$, the tables behave in very different ways. The locked table and the linear probe table have basically constant performance, while the lfc table drops off somewhat for $P=2,4,8$ (but not 1) and the split table increases drastically in performance until $P=32$ or so, after which point it levels off. The behavior of the split table is easy to explain -- if we start with more elements loaded before timing begins, more of its indices from the initial table capacity of $16$ are initialized, which avoids method calls having to do the initialization themselves. 

The lfc table is a bit harder to explain. It's possible that the preloaded values just gives it more stuff to move around during the resize operations, but then the effect would've shown up on the locked table as well. It's also pretty strange how the dropoff occurs at 2 rather than 1 -- an extra packet really shouldn't make THAT much of a difference. I'm lacking in an explanation for this -- it might even just be some load issue on research2, since the write and read configs were tested in close temporal proximity. 
\subsection*{Read config speedups}
Graphs, for the four values of $\rho$. $0.5$ on top left, $0.75$ on top right, $0.9$ on bottom left, and $0.99$ on bottom right. $n$ on the horizontal axis, speedup on the vertical.

\includegraphics[width=0.5\textwidth]{img/reads_5.png}
\includegraphics[width=0.5\textwidth]{img/reads_75.png}

\includegraphics[width=0.5\textwidth]{img/reads_9.png}
\includegraphics[width=0.5\textwidth]{img/reads_99.png}

$\rho$ doesn't seem to have much effect at all. This is not surprising, as the resizing policy prevents contains from searching too far for an element it can't find. If we actually have something uniformly distributed, the number of buckets with one or two elements should constitute the vast majority of the buckets (in a steady-state), which means that a contains call would take the same amount of time regardless of whether the key was in the table.

Among the tables, the lfc table seems to be consistently the highest-performing table, except in the case of $n=2$, where the split table has a minor advantage. Once again, following the reasoning from the parallel overhead section, lfc has less overhead in the contains call, so it makes sense that it would perform better when the vast majority of operations are contains calls.

In all cases, the linear probe table suffers a large dropoff at $n=7$ for some reason. This is not due to load factors -- the loop over $\rho$ is the outer loop in the testing script, so there is a systematic effect. After some explorative testing with the performance of \verb|pthread_rwlock|, I found that it has a dip in performance for $n=7$ but rises on $n=15$ when multiple threads are trying to acquire the read lock and release it in a loop. Since every contains call is bracketed by a pair of these calls, it seems that this may be the reason for the dip.

Throughputs for the rwlock read acquisition/release are as follows:

\begin{tabular}{ccccc}
    1&2&3&7&15\\
    \hline
    18851&3700&1491&1071&3348\\
\end{tabular}

\subsection*{Write config speedups}
Graphs, arranged the same way as in the read configuration.

\includegraphics[width=0.5\textwidth]{img/writes_5.png}
\includegraphics[width=0.5\textwidth]{img/writes_75.png}

\includegraphics[width=0.5\textwidth]{img/writes_9.png}
\includegraphics[width=0.5\textwidth]{img/writes_99.png}

Here, the split table and the lock-free contains table are pretty much on par with each other, with the split table having a slight edge. The locking table trails behind significantly, with the probe table still coming in dead last with the same pattern as before (probably due to the read-write lock again). 

Originally, I had expected the table without locks on add/remove to work faster than the one with. However, it doesn't seem like this is the case, at least not markedly so. There are two possible reasons for this. First, adds call malloc every single time, and if the performance of that function degrades a lot under heavy parallelism, then this would even out the performance between the two. We can actually test whether this is the case by preallocating some large number of nodes for each thread to draw from so that they don't have to call malloc.

The other possibility is that the add function in the split table has a lot of failure modes -- it has to start over again whenever something gets changed from out under it. This seems like a pretty unlikely scenario given the large number of buckets, but it's hard to tell.

In any case, it may even be that the lock-free approach offers no advantages over the lock-free contains. If contention is low, then it may be that the effort of acquiring the lock is lower than the effort of constantly checking to see if something had been changed from out under you, which would make the locking approach more attractive both due to simplicity and performance.

Finally, these speedups may look disappointing, but the maximum theoretical speedup is the dispatcher rate divided by the serial throughput. This is around $2.5$ for both reads and writes, which means that these tables are getting pretty close, at least in the case of reads.
\end{document}
